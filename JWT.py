# (DPX_ARMY_FF_01) Thank you from me to the haters
import os
import json
import aiohttp
import asyncio
import time
import logging
import traceback
import base64
import re
import shutil # Added for file copying
from datetime import datetime, timedelta, timezone
from html import escape
from collections import defaultdict

# Telegram Bot Library Imports
from telegram import Update, InputFile, ReplyKeyboardMarkup, ReplyKeyboardRemove
from telegram.ext import (
    Application,
    CommandHandler,
    MessageHandler,
    filters,
    CallbackContext,
    ContextTypes,
    # ConversationHandler, # Not using complex ConversationHandler for now
)
from telegram.constants import ParseMode
from telegram.error import TelegramError, Forbidden, BadRequest

# Environment Variable Loading
from dotenv import load_dotenv

# --- Configuration ---
load_dotenv()

# Essential: Get Bot Token
TOKEN = os.getenv('8486810316:AAGuKKXJz17UHrO-2_-SnAq6R8GBVU6sagk') # Prefer .env
if not TOKEN or TOKEN == "8486810316:AAGuKKXJz17UHrO-2_-SnAq6R8GBVU6sagk": # Check if token is missing or placeholder
    # !!! Replace with your actual bot token if not using .env !!!
    TOKEN = "8486810316:AAGuKKXJz17UHrO-2_-SnAq6R8GBVU6sagk" # FALLBACK - Highly recommended to use .env

# Optional: API Configuration
API_BASE_URL = os.getenv('JWT_API_URL', 'https://xanf-jwt-tocken-api.vercel.app/token')#∆ä∆†∆ù∆¨ ∆á”á∆õ∆ù∆ì–Ñ ∆†∆¨”á–Ñ∆¶∆ú∆ñ∆ß–Ñ –Ñ∆¶∆¶∆†∆¶
API_KEY = os.getenv('JWT_API_KEY', '@XAHAF_LEGACY')

# Optional: Bot Settings
MAX_FILE_SIZE = int(os.getenv('MAX_FILE_SIZE', 5 * 1024 * 1024))  # 5MB default
ADMIN_ID = int(os.getenv('ADMIN_ID', 6729993868)) # Default to 0 (disabled) if not set or invalid
# Check and warn if ADMIN_ID is 0 but commands are expected
if ADMIN_ID == 6729993868:
    print("WARNING: ADMIN_ID environment variable is not set, invalid, or 0. Admin commands (/vip, /broadcast) and error forwarding will be disabled.")
MAX_CONCURRENT_REQUESTS = int(os.getenv('MAX_CONCURRENT_REQUESTS', 30)) # Limit concurrent API requests
ADMIN_CONTACT_LINK = os.getenv('ADMIN_CONTACT_LINK', 'https://t.me/XAHAF_LEGACY') # Default admin contact link
AUTO_PROCESS_CHECK_INTERVAL = int(os.getenv('AUTO_PROCESS_CHECK_INTERVAL', 60)) # Seconds between scheduler checks (min 60 recommended)

# --- File Paths ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, 'bot_data')
TEMP_DIR = os.path.join(DATA_DIR, 'temp_files')
SCHEDULED_FILES_DATA_DIR = os.path.join(DATA_DIR, 'scheduled_files_data') # New directory for storing scheduled files

VIP_FILE = os.path.join(DATA_DIR, 'vip_users.json')
GITHUB_CONFIG_FILE = os.path.join(DATA_DIR, 'githubconfigs.json')
KNOWN_USERS_FILE = os.path.join(DATA_DIR, 'knownusers.json')
SCHEDULED_FILES_CONFIG = os.path.join(DATA_DIR, 'scheduledfiles.json') # New config file

# --- Logging Setup ---
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
# Reduce log noise from libraries
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("telegram.vendor.ptb_urllib3.urllib3").setLevel(logging.WARNING)
logger = logging.getLogger(__name__)

# --- Helper Functions ---

def format_time(seconds: float) -> str:
    """Formats seconds into a human-readable HH:MM:SS string."""
    if seconds is None or seconds < 0: return "N/A"
    try:
        seconds_int = int(seconds)
        if seconds_int < 60:
            # Handle cases like 0 seconds correctly
            return f"{seconds_int}s" if seconds_int >= 0 else "0s"
        delta = timedelta(seconds=seconds_int)
        total_seconds = delta.total_seconds()
        hours, remainder = divmod(total_seconds, 3600)
        minutes, seconds = divmod(remainder, 60)

        parts = []
        if hours > 0: parts.append(f"{int(hours)}h")
        if minutes > 0 or (hours > 0 and seconds > 0): parts.append(f"{int(minutes)}m")
        # Show seconds if it's the only unit or if there are minutes/hours
        if seconds > 0 or (not parts and total_seconds >=0): parts.append(f"{int(seconds)}s")

        if not parts: return "0s" # Should be caught above, but safety

        return " ".join(parts).strip()

    except (OverflowError, ValueError):
        return "Infinity"
    except Exception as e:
        logger.warning(f"Error formatting time {seconds}: {e}")
        return "Format Error"

def sanitize_filename(name: str) -> str:
    """Sanitizes a string to be used as part of a filename, ensuring it ends with .json."""
    if not name: return 'Unknown.json' # Ensure it always has an extension for later logic
    # Replace problematic characters with underscores
    # Allow alphanumeric, underscores, hyphens, and periods
    sanitized = re.sub(r'[^a-zA-Z0-9_.-]', '_', name)
    # Remove leading/trailing underscores/periods/hyphens
    sanitized = sanitized.strip(' _.-')
    # Ensure it ends with .json (case-insensitive check)
    if not sanitized.lower().endswith('.json'):
        # Remove existing extension if present before adding .json
        base, _ = os.path.splitext(sanitized)
        sanitized = base + ".json"

    # Ensure it's not empty after sanitization, default to 'Unknown.json'
    if not sanitized or sanitized == '.json':
        return 'Unknown.json'
    return sanitized

def parse_interval(interval_str: str) -> int | None:
    """Parses interval strings like '1h', '30m', '2d' into seconds."""
    match = re.match(r'^(\d+)\s*(m|h|d)$', interval_str.lower().strip())
    if not match:
        return None
    value, unit = int(match.group(1)), match.group(2)
    if unit == 'm':
        return value * 60
    elif unit == 'h':
        return value * 3600
    elif unit == 'd':
        return value * 86400
    return None # Should not happen with the regex

def load_json_data(filepath: str, default_value=None) -> dict | list:
    """Loads JSON data from a file, returning default_value on error or if file not found."""
    if default_value is None:
        default_value = {}
    try:
        dir_name = os.path.dirname(filepath)
        if dir_name: # Ensure directory exists before reading/writing
            os.makedirs(dir_name, exist_ok=True)

        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        logger.info(f"File {filepath} not found, creating with default value.")
        save_json_data(filepath, default_value) # Create the file with default
        return default_value
    except json.JSONDecodeError as e:
        logger.error(f"Error decoding JSON from {filepath}: {e}. Backing up corrupted file and returning default.")
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            corrupted_backup_path = f"{filepath}.corrupted_{timestamp}"
            os.rename(filepath, corrupted_backup_path)
            logger.info(f"Backed up corrupted file to {corrupted_backup_path}")
        except OSError as ren_err:
             logger.error(f"Could not backup corrupted file {filepath}: {ren_err}")
        # Create a new file with default value after backup attempt
        save_json_data(filepath, default_value)
        return default_value
    except Exception as e:
        logger.error(f"Unexpected error loading {filepath}: {e}. Returning default value.", exc_info=True)
        return default_value

def save_json_data(filepath: str, data: dict | list) -> bool:
    """Saves data to a JSON file using atomic write. Returns True on success, False on error."""
    temp_filepath = filepath + ".tmp"
    try:
        dir_name = os.path.dirname(filepath)
        if dir_name: # Ensure directory exists before writing
             os.makedirs(dir_name, exist_ok=True)

        # Write to temporary file first
        with open(temp_filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=4, ensure_ascii=False)

        # Atomically replace the original file with the temporary file
        os.replace(temp_filepath, filepath)
        logger.debug(f"Successfully saved data to {filepath}")
        return True
    except OSError as e:
        logger.error(f"OS Error saving data to {filepath}: {e}")
        return False
    except Exception as e:
        logger.error(f"Unexpected error saving data to {filepath}: {e}", exc_info=True)
        return False
    finally:
        # Ensure temporary file is removed if it still exists
        if os.path.exists(temp_filepath):
            try:
                os.remove(temp_filepath)
            except OSError as e:
                logger.warning(f"Could not remove temporary save file {temp_filepath}: {e}")

# --- VIP User Management ---
def load_vip_data() -> dict:
    """Loads VIP user data from the JSON file."""
    return load_json_data(VIP_FILE, {})

def save_vip_data(data: dict) -> bool:
    """Saves VIP user data to the JSON file."""
    return save_json_data(VIP_FILE, data)

def is_user_vip(user_id: int) -> bool:
    """Checks if a user is currently a VIP by verifying their expiry date."""
    vip_data = load_vip_data()
    user_id_str = str(user_id)
    if user_id_str in vip_data and isinstance(vip_data.get(user_id_str), dict):
        try:
            expiry_iso = vip_data[user_id_str].get('expiry')
            if expiry_iso:
                # Ensure correct timezone handling (assume stored is UTC)
                expiry_dt = datetime.fromisoformat(expiry_iso.replace('Z', '+00:00'))
                return expiry_dt > datetime.now(timezone.utc)
            else:
                # Missing expiry field means not VIP
                logger.debug(f"Missing or null 'expiry' for VIP user {user_id_str}. Assuming not VIP.")
                return False
        except (ValueError, KeyError, TypeError) as e:
            # Handle invalid data format gracefully
            logger.warning(f"Invalid or missing VIP data format for user {user_id_str}: {e}. Assuming not VIP.")
            return False
    return False # Not in VIP data or data is not a dict

def get_vip_expiry(user_id: int) -> str | None:
    """Gets the VIP expiry date string (YYYY-MM-DD HH:MM:SS UTC) if the user is currently VIP."""
    vip_data = load_vip_data()
    user_id_str = str(user_id)
    if user_id_str in vip_data and isinstance(vip_data.get(user_id_str), dict):
        try:
            expiry_iso = vip_data[user_id_str].get('expiry')
            if expiry_iso:
                expiry_dt = datetime.fromisoformat(expiry_iso.replace('Z', '+00:00'))
                if expiry_dt > datetime.now(timezone.utc):
                    # Format for display
                    return expiry_dt.strftime('%Y-%m-%d %H:%M:%S UTC')
                else:
                    return None # Expired
            else:
                return None # No expiry date stored
        except (ValueError, KeyError, TypeError):
            # Handle case where data is corrupt
            return "Invalid Date Stored"
    return None # User not VIP or data missing

# --- GitHub Config Management ---
def load_github_configs() -> dict:
    """Loads GitHub configuration data."""
    return load_json_data(GITHUB_CONFIG_FILE, {})

def save_github_configs(data: dict) -> bool:
    """Saves GitHub configuration data."""
    return save_json_data(GITHUB_CONFIG_FILE, data)

# --- Known User Management (for Broadcast) ---
def load_known_users() -> set:
    """Loads known user IDs from the file into a set for efficient lookup."""
    user_list = load_json_data(KNOWN_USERS_FILE, []) # Default to empty list
    valid_users = set()
    if isinstance(user_list, list):
        for item in user_list:
            # Convert valid string IDs to int, ensure non-zero
            if isinstance(item, int) and item != 0:
                valid_users.add(item)
            elif isinstance(item, str) and item.isdigit() and int(item) != 0:
                 valid_users.add(int(item))
            # Silently ignore invalid entries
    else:
        # If file content is not a list, reset it
        logger.error(f"Loaded known users data from {KNOWN_USERS_FILE} is not a list. Resetting to empty list.")
        save_known_users(set()) # Save an empty list back
        return set()
    return valid_users

def save_known_users(user_set: set) -> bool:
    """Saves the set of user IDs back to the file as a sorted list of integers."""
    # Ensure only valid integer IDs are saved, sorted
    int_user_list = sorted([int(uid) for uid in user_set if isinstance(uid, (int, str)) and str(uid).isdigit() and int(str(uid)) != 0])
    return save_json_data(KNOWN_USERS_FILE, int_user_list)

def add_known_user(user_id: int) -> None:
    """Adds a user ID to the known users file if not already present."""
    if not isinstance(user_id, int) or user_id == 0:
        logger.debug(f"Attempted to add invalid user ID: {user_id}. Skipping.")
        return
    known_users = load_known_users()
    if user_id not in known_users:
        known_users.add(user_id)
        if save_known_users(known_users):
             logger.info(f"Added new user {user_id} to known users list ({len(known_users)} total).")
        else:
             # Log error but don't crash the bot
             logger.error(f"Failed attempt to save known users file after adding {user_id}.")

# --- Scheduled File Management (NEW) ---
def load_scheduled_files() -> dict:
    """Loads scheduled file configurations."""
    # Structure: { "user_id_str": { "schedule_name.json": { schedule_details... }, ... }, ... }
    return load_json_data(SCHEDULED_FILES_CONFIG, {})

def save_scheduled_files(data: dict) -> bool:
    """Saves scheduled file configurations."""
    return save_json_data(SCHEDULED_FILES_CONFIG, data)

# --- Command Buttons ---
COMMAND_BUTTONS_LAYOUT = [
    ["Process File üì§", "Vip Status üìá"],
    ["Vip Shop üõí", "GitHub Status üìä"],
    ["Scheduled Files ‚öôÔ∏è", "Help üÜò"], # Added Scheduled Files button
    ["Cancel ‚ùå"]
]
main_reply_markup = ReplyKeyboardMarkup(COMMAND_BUTTONS_LAYOUT, resize_keyboard=True, one_time_keyboard=False)

# --- Bot Command Handlers ---

async def start(update: Update, context: CallbackContext) -> None:
    """Send welcome message with buttons and record the user."""
    user = update.effective_user
    if not user: return
    add_known_user(user.id)
    # Clear any potentially stuck pending schedule state on /start
    context.user_data.pop('pending_schedule', None)
    context.user_data.pop('waiting_for_json', None) # Also clear manual state

    username = escape(user.first_name) or "there" # Escape name for safety

    start_msg = f"üëã Hello {username}!\n\n"
    start_msg += "üöÄ Welcome to the JWT Token Generator Bot!\n\n"
    start_msg += "üìÅ Send me a JSON file containing account credentials like this:\n"
    start_msg += "```json\n"
    start_msg += '[\n'
    start_msg += '    {"uid": "user1", "password": "pass1"},\n'
    start_msg += '    {"uid": "user2", "password": "pass2"}\n'
    start_msg += '    // ... more entries ...\n'
    start_msg += ']\n'
    start_msg += "```\n"
    start_msg += "‚úÖ Successful tokens (Region summary included in message) will be saved to `jwt_token.json` AND `accounts{Region}.json` files.\n"
    start_msg += "‚úîÔ∏è Working accounts (UID/Pass) will be saved to `working_account.json`\n"
    start_msg += "‚ùå Failed/invalid entries (UID/Pass) will be saved to `lost_account.json`\n\n"
    start_msg += f"‚ö†Ô∏è Max file size: {MAX_FILE_SIZE / 1024 / 1024:.1f}MB\n\n"
    start_msg += "‚ú® *VIP Features:*\n"
    start_msg += "  - Auto-upload tokens to GitHub.\n"
    start_msg += "  - Schedule files for automatic periodic processing and GitHub upload (`/setfile`).\n\n"
    start_msg += "Use /help or the Help button (üÜò) to see all available commands."

    await update.message.reply_text(
        start_msg,
        reply_markup=main_reply_markup,
        parse_mode=ParseMode.MARKDOWN
    )

async def help_command(update: Update, context: CallbackContext) -> None:
    """Send the detailed help message."""
    user = update.effective_user
    if user: add_known_user(user.id)
    # Clear any pending schedule state on /help
    context.user_data.pop('pending_schedule', None)
    context.user_data.pop('waiting_for_json', None) # Also clear manual state

    help_text = (
        "üÜò *Help Center*\n\n"
        "üìå *Available Commands:*\n"
        "  `/start` - Show the main welcome message\n"
        "  `/help` - Show this help message\n"
        "  `/vipstatus` - Check your current VIP status\n"
        "  `/vipshop` - View available VIP plans\n"
        "  `/cancel` - Cancel the current operation (like setting a scheduled file)\n\n"
        "üîß *VIP Commands (for VIP users only):*\n"
        "  `/setgithub <TOKEN> <owner/repo> <branch> <filename.json>` - Configure GitHub auto-upload.\n"
        "  `/mygithub` - Show your current GitHub configuration.\n"
        "  `/setfile <Interval> <ScheduleName.json>` - Start scheduling a file for auto-processing (e.g., `/setfile 12h my_accounts.json`). Bot will ask for the file.\n"
        "     *Interval format:* `Xm` (minutes), `Xh` (hours), `Xd` (days). Min interval: 5m.\n"
        "  `/removefile <ScheduleName.json>` - Stop auto-processing for a scheduled file.\n"
        "  `/scheduledfiles` - List your currently scheduled files.\n\n"
        "üëë *Admin Commands (Bot Admin only - requires ADMIN_ID to be set):*\n"
        "  `/vip add <user_id> <days>` - Add/extend VIP\n"
        "  `/vip remove <user_id>` - Remove VIP, GitHub config & ALL user's scheduled files\n"
        "  `/vip list` - Show active VIP users\n"
        "  `/broadcast <message>` - Send a message to all known users\n\n"
        "üì§ *Manual Processing:*\n"
        "  1. Send a JSON file formatted with UID-password pairs.\n"
        "  2. The bot processes it and returns result files.\n"
        "  3. VIPs with GitHub config get `jwt_token.json` uploaded.\n\n"
        "‚öôÔ∏è *Automatic Processing (VIP):*\n"
        "  1. Use `/setfile` to define a schedule and name.\n"
        "  2. Send the corresponding JSON file when prompted.\n"
        "  3. The bot will automatically process this file at the set interval and upload tokens to GitHub if configured.\n"
    )
    await update.message.reply_text(
        help_text,
        parse_mode=ParseMode.MARKDOWN,
        reply_markup=main_reply_markup,
        disable_web_page_preview=True
    )

async def vip_shop_command(update: Update, context: CallbackContext) -> None:
    """Displays the VIP subscription plans and contact info."""
    user = update.effective_user
    if user: add_known_user(user.id)
    context.user_data.pop('pending_schedule', None) # Clear state
    context.user_data.pop('waiting_for_json', None)

    vip_shop_text = (
        "‚ú® Unlock **Automatic GitHub Uploads** & **Scheduled File Processing** ‚ú®\n"
        "& Other Premium Features Instantly!\n\n"
        "üíº *Available Plans & Prices:*\n\n"
        "üóìÔ∏è   `7 Days`       ‚Äî   `‚Çπ 10`\n"
        "üóìÔ∏è  `15 Days`     ‚Äî   `‚Çπ 49`\n"
        "üìÖ  `1 Month`      ‚Äî   `‚Çπ 69`\n"
        "üìÖ  `2 Months`     ‚Äî   `‚Çπ 89`\n"
        "üìÖ  `3 Months`     ‚Äî   `‚Çπ 99`\n"
        "üéØ  `1 Year`       ‚Äî   `‚Çπ 159`\n\n"
        "üì© *To Purchase VIP Membership:*\n"
        f"Contact Admin üëâ [Admin Contact]({ADMIN_CONTACT_LINK})"
    )

    await update.message.reply_text(
        vip_shop_text,
        parse_mode=ParseMode.MARKDOWN,
        reply_markup=main_reply_markup,
        disable_web_page_preview=True
    )

async def vip_status_command(update: Update, context: CallbackContext) -> None:
    """Shows the user's current VIP status and expiry date."""
    user = update.effective_user
    if not user: return
    user_id = user.id
    add_known_user(user_id)
    context.user_data.pop('pending_schedule', None) # Clear state
    context.user_data.pop('waiting_for_json', None)

    expiry_date_str = get_vip_expiry(user_id)

    if expiry_date_str and "Invalid" not in expiry_date_str:
        status_msg = f"üåü *VIP Status:* Active\n*Expires:* `{expiry_date_str}`"
    elif expiry_date_str == "Invalid Date Stored":
        status_msg = "‚ö†Ô∏è *VIP Status:* Error reading expiry date. Please contact admin."
    else:
        status_msg = "‚ÑπÔ∏è *Status:* Regular User\nUse /vipshop to upgrade and unlock premium features!"

    await update.message.reply_text(
        status_msg,
        parse_mode=ParseMode.MARKDOWN,
        reply_markup=main_reply_markup
    )

async def cancel(update: Update, context: CallbackContext) -> None:
    """Handles the /cancel command or Cancel button, clearing pending actions."""
    user = update.effective_user
    user_id = user.id if user else "Unknown"
    cleared_action = False
    if context.user_data.pop('pending_schedule', None):
        cleared_action = True
        logger.info(f"User {user_id} cancelled pending file schedule setup.")
        await update.message.reply_text(
            "Scheduled file setup cancelled. Returning to main menu.",
            reply_markup=main_reply_markup
        )
    elif context.user_data.pop('waiting_for_json', None):
         cleared_action = True
         logger.info(f"User {user_id} cancelled waiting for manual JSON process.")
         await update.message.reply_text(
            "Waiting for manual process file cancelled. Returning to main menu.",
            reply_markup=main_reply_markup
         )
    # Add more elif blocks here if other states are introduced

    if not cleared_action:
        logger.info(f"User {user_id} used /cancel, but no active operation found.")
        await update.message.reply_text(
            "No active operation to cancel. Returning to main menu.",
            reply_markup=main_reply_markup
        )

# --- File Processing Logic ---

async def process_account(session: aiohttp.ClientSession, account: dict, semaphore: asyncio.Semaphore) -> tuple[str | None, str | None, dict | None, dict | None, str | None]:
    """
    Processes a single account via the API to get a JWT token and potentially region.
    (Remains mostly the same, but added slightly more robust logging)
    Returns: tuple(token | None, region | None, working_account | None, lost_account | None, error_reason | None)
    """
    uid = account.get("uid")
    password = account.get("password")
    error_reason = None
    # Keep original structure exactly as provided in the input file for working/lost lists
    original_account_info = account.copy()

    if not uid: error_reason = "Missing 'uid'"
    elif not password: error_reason = "Missing 'password'"

    if error_reason:
        logger.debug(f"Skipping account due to validation error: {error_reason} - Account: {account}")
        # Return original account info in the 'lost' part for consistency
        lost_info = {**original_account_info, "error_reason": error_reason}
        return None, None, None, lost_info, error_reason

    uid_str = str(uid) # Ensure UID is string for API call

    async with semaphore:
        # Construct URL carefully, ensuring proper encoding (aiohttp handles this)
        params = {'uid': uid_str, 'password': password, 'key': API_KEY}
        try:
            # Increased timeout slightly for potentially slower API responses
            async with session.get(API_BASE_URL, params=params, timeout=aiohttp.ClientTimeout(total=60)) as response:
                response_text = await response.text()

                if 200 <= response.status < 300:
                    try:
                        result = json.loads(response_text)
                        # Check structure more carefully
                        if isinstance(result, dict) and result.get('token'):
                            token = result['token']
                            # Region is optional but useful
                            region = result.get('region') # Can be None
                            logger.info(f"Success: Token received for UID: {uid_str} (Region: {region})")
                            # Return original account info for working list
                            return token, region, original_account_info, None, None
                        else:
                            err_msg = "API OK but invalid response format or empty token"
                            logger.warning(f"{err_msg} for UID: {uid_str}. Response: {response_text[:200]}")
                            lost_info = {**original_account_info, "error_reason": err_msg}
                            return None, None, None, lost_info, err_msg
                    except json.JSONDecodeError:
                        err_msg = f"API OK ({response.status}) but Non-JSON response"
                        logger.error(f"{err_msg} for UID: {uid_str}. Response: {response_text[:200]}")
                        lost_info = {**original_account_info, "error_reason": err_msg}
                        return None, None, None, lost_info, err_msg
                    except Exception as e: # Catch potential errors during result processing
                         err_msg = f"API OK ({response.status}) but response parsing error: {e}"
                         logger.error(f"{err_msg} for UID: {uid_str}", exc_info=True)
                         lost_info = {**original_account_info, "error_reason": err_msg}
                         return None, None, None, lost_info, err_msg

                else: # Handle non-2xx status codes
                    # Attempt to extract a more meaningful error from API response
                    error_detail = f"API Error ({response.status})"
                    try:
                        error_json = json.loads(response_text)
                        if isinstance(error_json, dict):
                            # Look for common error message keys
                            msg = error_json.get('message') or error_json.get('error') or error_json.get('detail')
                            if msg and isinstance(msg, str):
                                error_detail += f": {msg[:100]}" # Limit length
                    except (json.JSONDecodeError, TypeError): pass # Ignore if response isn't useful JSON

                    logger.warning(f"API Error for UID: {uid_str}. Status: {response.status}. Detail: {error_detail}. Raw Response: {response_text[:200]}")
                    lost_info = {**original_account_info, "error_reason": error_detail}
                    return None, None, None, lost_info, error_detail

        except asyncio.TimeoutError:
             logger.warning(f"Timeout processing API request for UID: {uid_str}")
             error_reason = "Request Timeout"
             lost_info = {**original_account_info, "error_reason": error_reason}
             return None, None, None, lost_info, error_reason
        except aiohttp.ClientConnectorError as e:
             # More specific logging for network errors
             logger.error(f"Network Connection Error processing UID {uid_str}: {e}")
             error_reason = f"Network Error: {e}"
             lost_info = {**original_account_info, "error_reason": error_reason}
             return None, None, None, lost_info, error_reason
        except aiohttp.ClientError as e:
             # Catch other potential client errors
             logger.error(f"AIOHTTP Client Error processing UID {uid_str}: {e}")
             error_reason = f"HTTP Client Error: {e}"
             lost_info = {**original_account_info, "error_reason": error_reason}
             return None, None, None, lost_info, error_reason

        # General catch-all for unexpected issues during the request
        except Exception as e:
             logger.error(f"Unexpected error processing UID {uid_str}: {e}", exc_info=True)
             error_reason = f"Unexpected Processing Error: {e}"
             lost_info = {**original_account_info, "error_reason": error_reason}
             return None, None, None, lost_info, error_reason

async def handle_document(update: Update, context: CallbackContext) -> None:
    """Handle incoming JSON documents OR files sent after /setfile."""
    user = update.effective_user
    message = update.message
    if not user or not message: return
    user_id = user.id
    chat_id = message.chat_id
    add_known_user(user.id)

    # --- Check if this file is for a pending schedule ---
    if context.user_data.get('pending_schedule'):
        await handle_scheduled_file_upload(update, context)
        return # Stop further processing here if it was for a schedule

    # --- Standard Manual File Processing ---
    process_button_text = COMMAND_BUTTONS_LAYOUT[0][0]
    if message.text == process_button_text and not message.document:
        await message.reply_text(
            "Okay, please send the JSON file now for manual processing.\n\n"
            "Make sure it's a `.json` file containing a list like:\n"
            "```json\n"
            '[\n  {"uid": "user1", "password": "pass1"},\n  {"uid": "user2", "password": "pass2"}\n]\n'
            "```",
            parse_mode=ParseMode.MARKDOWN,
            reply_markup=ReplyKeyboardRemove() # Temporarily remove keyboard
        )
        context.user_data['waiting_for_json'] = True # State for manual processing
        return

    was_waiting_manual = context.user_data.pop('waiting_for_json', False)
    if was_waiting_manual and not message.document:
         await message.reply_text("Looks like you sent text instead of a file for manual processing. Please send the JSON file or use /cancel.", reply_markup=main_reply_markup)
         return
    elif not was_waiting_manual and not message.document:
        # Ignore text messages that aren't commands or button clicks handled elsewhere
        # unless it's part of a conversation (which we aren't using heavily here yet)
        # This prevents the forward_to_admin from catching simple text replies.
        # If more complex conversations are added, this might need adjustment.
        # Check if it's a known button text before returning
        known_button_texts = {btn for row in COMMAND_BUTTONS_LAYOUT for btn in row}
        if message.text not in known_button_texts:
            logger.debug(f"Ignoring unhandled text message from user {user_id} in private chat.")
            # Optionally, forward this to admin if desired using forward_to_admin logic
            # await forward_to_admin(update, context) # Uncomment to forward these too
        return

    document = message.document
    if not document: return # Should be caught above, but safety check

    # --- File Validation ---
    # Be stricter: Must have .json extension OR be application/json
    is_json_mime = document.mime_type and document.mime_type.lower() == 'application/json'
    has_json_extension = document.file_name and document.file_name.lower().endswith('.json')

    if not is_json_mime and not has_json_extension:
        await message.reply_text("‚ùå File does not appear to be a JSON file. Please ensure it has a `.json` extension or the correct `application/json` type.", reply_markup=main_reply_markup)
        return

    file_id = document.file_id
    file_name = document.file_name or f"file_{file_id}.json" # Fallback filename

    # Check file size before download if possible
    if document.file_size and document.file_size > MAX_FILE_SIZE:
        await message.reply_text(
            f"‚ö†Ô∏è File is too large ({document.file_size / 1024 / 1024:.2f} MB). Max: {MAX_FILE_SIZE / 1024 / 1024:.1f} MB.",
            reply_markup=main_reply_markup
        )
        return

    temp_file_path = os.path.join(TEMP_DIR, f'input_manual_{user_id}_{int(time.time())}.json')
    progress_message = None
    accounts_data = []

    # --- Download and Parse (Manual Processing) ---
    try:
        os.makedirs(TEMP_DIR, exist_ok=True)
        progress_message = await message.reply_text(f"‚è≥ Downloading `{escape(file_name)}` for manual processing...", parse_mode=ParseMode.MARKDOWN)

        bot_file = await context.bot.get_file(file_id)
        await bot_file.download_to_drive(temp_file_path)
        logger.info(f"User {user_id} uploaded file '{file_name}' for manual processing, downloaded to {temp_file_path}")

        await context.bot.edit_message_text(
            chat_id=progress_message.chat_id, message_id=progress_message.message_id,
            text=f"‚è≥ Downloaded `{escape(file_name)}`. Parsing JSON...", parse_mode=ParseMode.MARKDOWN
        )

        # Check size *after* download to be absolutely sure
        actual_size = os.path.getsize(temp_file_path)
        if actual_size > MAX_FILE_SIZE:
             raise ValueError(f"Downloaded file size ({actual_size / 1024 / 1024:.2f} MB) exceeds limit ({MAX_FILE_SIZE / 1024 / 1024:.1f} MB).")

        with open(temp_file_path, 'r', encoding='utf-8') as f:
            try:
                accounts_data = json.load(f)
            except json.JSONDecodeError as e:
                # Provide more context for JSON errors
                error_line_info = ""
                # Check if lineno/colno are available (added in Python 3.5)
                if hasattr(e, 'lineno') and hasattr(e, 'colno'):
                    error_line_info = f" near line {e.lineno}, column {e.colno}"
                error_msg = f"‚ùå Invalid JSON format in `{escape(file_name)}`{error_line_info}.\nError: `{escape(e.msg)}`.\nPlease check the file structure and syntax."
                await context.bot.edit_message_text(
                    chat_id=progress_message.chat_id, message_id=progress_message.message_id,
                    text=error_msg, parse_mode=ParseMode.MARKDOWN # No reply_markup on edit
                )
                # Notify admin about invalid JSON uploads
                if ADMIN_ID and ADMIN_ID != 0:
                    try:
                        await context.bot.send_message(ADMIN_ID, f"‚ö†Ô∏è User {user.id} uploaded invalid JSON for manual processing: `{escape(file_name)}`. Error: {escape(e.msg)}{error_line_info}")
                    except Exception as forward_e:
                        logger.error(f"Failed to forward invalid JSON notice to admin {ADMIN_ID}: {forward_e}")
                return # Stop processing

        # Validate JSON structure (must be a list of dictionaries)
        if not isinstance(accounts_data, list):
            raise ValueError("Input JSON structure is invalid. It must be an array (a list `[...]`) of objects.")
        if accounts_data and not all(isinstance(item, dict) for item in accounts_data):
             # Find first non-dict item for better error message
             first_bad_item = next((item for item in accounts_data if not isinstance(item, dict)), None)
             raise ValueError(f"All items inside the JSON array must be objects (`{{...}}`). Found an item that is not an object: `{escape(str(first_bad_item)[:50])}`...")

    except ValueError as e: # Catch our specific validation errors
        logger.warning(f"Input file validation failed for user {user_id} ('{file_name}'): {e}")
        error_text = f"‚ùå Validation Error: {escape(str(e))}"
        if progress_message:
             # Edit message without reply_markup
             await context.bot.edit_message_text(chat_id=progress_message.chat_id, message_id=progress_message.message_id, text=error_text, parse_mode=ParseMode.MARKDOWN)
        else:
             # Send new message with reply_markup
             await message.reply_text(error_text, reply_markup=main_reply_markup, parse_mode=ParseMode.MARKDOWN)
        return
    except TelegramError as e:
        logger.error(f"Telegram API error during file handling for user {user_id}: {e}")
        try:
            error_text = f"‚ö†Ô∏è A Telegram error occurred: `{escape(str(e))}`. Please try again later."
            if progress_message:
                await context.bot.edit_message_text(chat_id=progress_message.chat_id, message_id=progress_message.message_id, text=error_text, parse_mode=ParseMode.MARKDOWN)
            else:
                 await message.reply_text(error_text, reply_markup=main_reply_markup, parse_mode=ParseMode.MARKDOWN)
        except TelegramError: # If we can't even edit/send the error
            logger.error(f"Could not inform user {user_id} about Telegram error: {e}")
        return
    except Exception as e: # General catch-all
        logger.error(f"Error downloading or parsing file from user {user_id}: {e}", exc_info=True)
        error_text = f"‚ö†Ô∏è An unexpected error occurred while handling the file. Please try again or contact admin if it persists."
        if progress_message:
            try: # Try editing the progress message first
                await context.bot.edit_message_text(chat_id=progress_message.chat_id, message_id=progress_message.message_id, text=error_text) # Removed reply_markup
            except TelegramError: # If that fails, send a new message
                await message.reply_text(error_text, reply_markup=main_reply_markup)
        else: # If no progress message existed
            await message.reply_text(error_text, reply_markup=main_reply_markup)
        return
    finally:
        # Clean up the temporary downloaded file
        if os.path.exists(temp_file_path):
             try:
                 os.remove(temp_file_path)
             except OSError as e:
                 logger.warning(f"Could not remove temp input file {temp_file_path}: {e}")

    # --- Process Accounts (Manual Processing) ---
    total_count = len(accounts_data)
    if total_count == 0:
        # Edit the progress message
        await context.bot.edit_message_text(
            chat_id=progress_message.chat_id, message_id=progress_message.message_id,
            text="‚ÑπÔ∏è The provided JSON file is empty or contains no valid account objects." # Removed reply_markup
        )
        return

    # Edit progress message
    await context.bot.edit_message_text(
        chat_id=progress_message.chat_id, message_id=progress_message.message_id,
        text=f"üîÑ *Processing {total_count} Accounts (Manual)*\nInitializing API calls (max {MAX_CONCURRENT_REQUESTS} parallel)...",
        parse_mode=ParseMode.MARKDOWN
    )

    start_time = time.time()
    processed_count = 0
    successful_tokens = [] # Store dicts like {"token": "...", "region": "..."}
    working_accounts = []
    lost_accounts = []
    errors_summary = defaultdict(int) # Use defaultdict for easier counting

    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
    async with aiohttp.ClientSession() as session:
        tasks = [process_account(session, account, semaphore) for account in accounts_data]
        last_update_time = time.time()
        last_progress_text_sent = "" # Track last sent text to avoid spamming "not modified"

        for future in asyncio.as_completed(tasks):
            try:
                token, region, working_acc, lost_acc, error_reason = await future
            except Exception as task_err:
                # Log errors from the task itself
                logger.error(f"Error retrieving result from processing task: {task_err}", exc_info=True)
                error_msg = f"Internal task error: {task_err}"
                # Try to associate with an account if possible, otherwise generic lost entry
                # Use lost_acc or working_acc if available from partial results before error
                lost_account_info = lost_acc or working_acc or {"uid": "unknown", "password": "unknown"}
                lost_accounts.append({**lost_account_info, "error_reason": error_msg})
                errors_summary[error_msg] += 1
                processed_count += 1
                continue # Move to the next task result

            processed_count += 1

            # Process results
            if token and working_acc:
                successful_tokens.append({"token": token, "region": region}) # Store region internally
                working_accounts.append(working_acc)
            elif lost_acc:
                lost_accounts.append(lost_acc)
                reason = lost_acc.get("error_reason", "Unknown Failure")
                # Simplify error reason for summary (e.g., group all 'API Error (404)' together)
                simple_error = reason.split(':')[0].strip()
                errors_summary[simple_error] += 1
            else:
                 # This case should ideally not happen if process_account is robust
                 logger.error(f"Task completed unexpectedly. Token:{token}, Region:{region}, Work:{working_acc}, Lost:{lost_acc}, Err:{error_reason}")
                 generic_lost_info = {"account_info": lost_acc or working_acc or "unknown", "error_reason": "Processing function returned unexpected state"}
                 lost_accounts.append(generic_lost_info)
                 errors_summary["Processing function error"] += 1


            # --- Progress Update ---
            current_time = time.time()
            # Update progress every ~2 seconds OR every N items OR on the last item
            update_frequency_items = max(10, min(100, total_count // 10)) # Scale frequency with total size
            time_elapsed_since_last_update = current_time - last_update_time;

            if time_elapsed_since_last_update > 2.0 or \
               (update_frequency_items > 0 and processed_count % update_frequency_items == 0) or \
               processed_count == total_count:

                elapsed_time = current_time - start_time
                percentage = (processed_count / total_count) * 100 if total_count > 0 else 0

                # Estimate remaining time (more reliable after a few items)
                estimated_remaining_time = -1 # Default to N/A
                if processed_count > 5 and elapsed_time > 2: # Avoid division by zero or early estimates
                    try:
                        time_per_item = elapsed_time / processed_count
                        remaining_items = total_count - processed_count
                        estimated_remaining_time = time_per_item * remaining_items
                    except ZeroDivisionError: pass # Should not happen with check above

                # Build progress text
                progress_text = (
                    f"üîÑ *Processing Accounts (Manual)...*\n\n"
                    f"Progress: {processed_count}/{total_count} ({percentage:.1f}%)\n"
                    f"‚úÖ Success: {len(successful_tokens)} | ‚ùå Failed: {len(lost_accounts)}\n"
                    f"‚è±Ô∏è Elapsed: {format_time(elapsed_time)}\n"
                    # Format time handles None/<0 correctly
                    f"‚è≥ Est. Remaining: {format_time(estimated_remaining_time)}"
                )

                # Send update only if text changed to avoid "Message is not modified" errors
                if last_progress_text_sent != progress_text:
                    try:
                        await context.bot.edit_message_text(
                            chat_id=progress_message.chat_id, message_id=progress_message.message_id,
                            text=progress_text, parse_mode=ParseMode.MARKDOWN
                        )
                        last_progress_text_sent = progress_text # Update last sent text
                        last_update_time = current_time # Reset timer
                    except TelegramError as edit_err:
                        # Ignore "not modified" errors, log others
                        if "Message is not modified" not in str(edit_err):
                             logger.warning(f"Could not edit progress message: {edit_err}")
                        last_update_time = current_time # Still update time to prevent rapid retries

    # --- Final Summary & File Generation (Manual Processing) ---
    final_elapsed_time = time.time() - start_time
    escaped_file_name = escape(file_name)
    final_summary_parts = [
        f"üèÅ *Manual Processing Complete for `{escaped_file_name}`*\n",
        f"üìä Total Accounts Processed: {total_count}",
        f"‚úÖ Successful Tokens: {len(successful_tokens)}",
        f"‚ùå Failed/Invalid Accounts: {len(lost_accounts)}",
        f"‚è±Ô∏è Total Time Taken: {format_time(final_elapsed_time)}"
    ]

    # Add Region Summary (uses successful_tokens which includes region)
    successful_by_region = defaultdict(list) # Use defaultdict
    if successful_tokens:
        for token_entry in successful_tokens:
            region = token_entry.get('region')
            # Group null/empty regions under "Unknown Region"
            region_name = region if region else "Unknown Region"
            successful_by_region[region_name].append(token_entry) # Append the whole entry

        if successful_by_region:
            final_summary_parts.append("\n*Successful by Region:*")
            # Sort regions alphabetically for consistent display
            sorted_regions = sorted(successful_by_region.keys())
            for region in sorted_regions:
                count = len(successful_by_region[region])
                final_summary_parts.append(f"- {escape(region)}: {count} tokens")
    else:
        # Explicitly state if no successful tokens were found
        final_summary_parts.append("\n*Successful by Region:* 0 tokens found.")


    # Add Error Summary
    if errors_summary:
        final_summary_parts.append("\n*Error Summary (Top 5 Types):*")
        # Sort errors by count descending
        sorted_errors = sorted(errors_summary.items(), key=lambda item: item[1], reverse=True)
        for msg, count in sorted_errors[:5]: # Show top 5
            final_summary_parts.append(f"- `{escape(msg)}`: {count} times")
        if len(sorted_errors) > 5:
            final_summary_parts.append(f"... and {len(sorted_errors) - 5} more error types.")

    final_summary = "\n".join(final_summary_parts)

    # ******** FIX: Send final summary as NEW message with keyboard ********
    try:
        # Delete the "Processing..." message first
        if progress_message:
            await context.bot.delete_message(chat_id=progress_message.chat_id, message_id=progress_message.message_id)
        # Send the summary as a new message with the main keyboard
        await message.reply_text(
            final_summary,
            parse_mode=ParseMode.MARKDOWN,
            reply_markup=main_reply_markup # Restore main keyboard here
        )
    except TelegramError as final_msg_err:
        logger.error(f"Could not delete progress message or send final summary: {final_msg_err}. Progress message ID: {progress_message.message_id if progress_message else 'N/A'}")
        # Fallback: Try sending summary again without deleting if delete failed
        try:
            await message.reply_text(
                final_summary,
                parse_mode=ParseMode.MARKDOWN,
                reply_markup=main_reply_markup
            )
        except Exception as fallback_err:
            logger.critical(f"Failed even fallback sending final summary for manual process: {fallback_err}")


    # --- Generate and Send Output Files ---
    # (This part remains the same, uses message.reply_document which is fine)
    file_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_files_to_send = [] # List of tuples: (temp_path, desired_filename_for_user)
    cleanup_paths = [] # List of temp file paths to delete later
    jwt_token_path_for_upload = None # Store path of the main JWT file for potential GitHub upload

    try:
        os.makedirs(TEMP_DIR, exist_ok=True) # Ensure temp dir exists

        # --- Create the main jwt_token.json file (tokens only) ---
        if successful_tokens:
            # Use a unique temp name before saving
            jwt_token_path = os.path.join(TEMP_DIR, f'jwt_only_{user_id}_{file_timestamp}.json')
            # Prepare list of ONLY tokens for the file
            tokens_only_list_for_file = [{"token": entry.get("token")} for entry in successful_tokens if entry.get("token")]

            if tokens_only_list_for_file: # Only save if there are tokens
                if save_json_data(jwt_token_path, tokens_only_list_for_file):
                    output_files_to_send.append((jwt_token_path, 'jwt_token.json'))
                    cleanup_paths.append(jwt_token_path)
                    jwt_token_path_for_upload = jwt_token_path # This file is used for GitHub upload
                else:
                    await message.reply_text("‚ö†Ô∏è Error saving main `jwt_token.json` to temporary storage.")
            else:
                 # This case might occur if processing succeeded but yielded empty tokens somehow
                 logger.warning(f"User {user_id}: No valid tokens found to save in main jwt_token.json, despite {len(successful_tokens)} successes reported.")

        # --- Create region-specific accounts{Region}.json files (tokens only) ---
        if successful_by_region: # Use the defaultdict created for the summary
            logger.info(f"Creating region-specific files for user {user_id}")
            for region_name, entries in successful_by_region.items():
                 if not entries: continue # Skip empty regions (shouldn't happen with defaultdict)

                 # Prepare list of tokens ONLY for this region's file
                 region_tokens_only = [{"token": entry.get("token")} for entry in entries if entry.get("token")]

                 if region_tokens_only: # Only create file if there are tokens for this region
                     sanitized_region_name = sanitize_filename(region_name) # Sanitize region name for filename
                     # Construct filename like accountsNA.json, accountsEU.json etc.
                     # Remove the .json part added by sanitize_filename for this specific format
                     base_region_name = os.path.splitext(sanitized_region_name)[0]
                     region_file_name = f'accounts{base_region_name}.json'

                     region_file_path = os.path.join(TEMP_DIR, f'{base_region_name}_{user_id}_{file_timestamp}.json') # Temp path

                     if save_json_data(region_file_path, region_tokens_only): # Save region-specific tokens list
                         output_files_to_send.append((region_file_path, region_file_name))
                         cleanup_paths.append(region_file_path)
                         logger.debug(f"Created region file: {region_file_name} with {len(region_tokens_only)} tokens.")
                     else:
                         await message.reply_text(f"‚ö†Ô∏è Error saving region file `{escape(region_file_name)}` to temporary storage.", parse_mode=ParseMode.MARKDOWN)
                         logger.error(f"Failed to save region file {region_file_name} for user {user_id}")
                 else:
                     logger.debug(f"Region '{region_name}' had entries but no valid tokens found after filtering. Skipping region file.")


        # --- Create working_account.json ---
        if working_accounts:
            working_account_path = os.path.join(TEMP_DIR, f'working_{user_id}_{file_timestamp}.json')
            if save_json_data(working_account_path, working_accounts):
                output_files_to_send.append((working_account_path, 'working_account.json'))
                cleanup_paths.append(working_account_path)
            else:
                await message.reply_text("‚ö†Ô∏è Error saving `working_account.json` to temporary storage.")

        # --- Create lost_account.json ---
        if lost_accounts:
            lost_account_path = os.path.join(TEMP_DIR, f'lost_{user_id}_{file_timestamp}.json')
            if save_json_data(lost_account_path, lost_accounts):
                output_files_to_send.append((lost_account_path, 'lost_account.json'))
                cleanup_paths.append(lost_account_path)
            else:
                await message.reply_text("‚ö†Ô∏è Error saving `lost_account.json` to temporary storage.")

        # Send the generated files (main jwt_token.json, regional files, working, lost)
        if output_files_to_send:
            await message.reply_text(f"‚¨áÔ∏è Sending {len(output_files_to_send)} result file(s)...")
            # Sort files alphabetically by name for consistent ordering
            output_files_to_send.sort(key=lambda x: x[1])
            for temp_path, desired_filename in output_files_to_send:
                 if not os.path.exists(temp_path):
                     logger.error(f"Output file {temp_path} (for {desired_filename}) not found before sending.")
                     await message.reply_text(f"‚ö†Ô∏è Internal Error: Could not find `{escape(desired_filename)}` for sending.", parse_mode=ParseMode.MARKDOWN)
                     continue
                 try:
                     with open(temp_path, 'rb') as f:
                         await message.reply_document(
                             document=InputFile(f, filename=desired_filename),
                             caption=f"`{escape(desired_filename)}`\nFrom manual processing of: `{escaped_file_name}`\nTotal Processed: {total_count}",
                             parse_mode=ParseMode.MARKDOWN
                         )
                     logger.info(f"Sent '{desired_filename}' to user {user_id} (manual process)")
                     await asyncio.sleep(0.5) # Small delay between sending files to avoid rate limits
                 except TelegramError as send_err:
                     logger.error(f"Failed to send '{desired_filename}' to user {user_id}: {send_err}")
                     await message.reply_text(f"‚ö†Ô∏è Failed to send `{escape(desired_filename)}`: {escape(str(send_err))}", parse_mode=ParseMode.MARKDOWN)
                 except Exception as general_err:
                     logger.error(f"Unexpected error sending '{desired_filename}' to {user_id}: {general_err}", exc_info=True)
                     await message.reply_text(f"‚ö†Ô∏è Unexpected error sending `{escape(desired_filename)}`.", parse_mode=ParseMode.MARKDOWN)
        elif total_count > 0:
             # Inform user if processing happened but no files were generated
             await message.reply_text("‚ÑπÔ∏è No output files were generated (e.g., 0 successful tokens found or error saving files).", reply_markup=main_reply_markup)

        # --- Trigger GitHub Auto-Upload if applicable ---
        # This section uses `jwt_token_path_for_upload`, which points to the main jwt_token.json file
        if is_user_vip(user_id) and jwt_token_path_for_upload:
            github_configs = load_github_configs()
            user_id_str = str(user_id)
            config = github_configs.get(user_id_str)

            if config and isinstance(config, dict):
                logger.info(f"User {user_id} is VIP with GitHub config. Attempting auto-upload (manual process).")
                if os.path.exists(jwt_token_path_for_upload):
                    # Use the background-compatible upload function
                    await upload_to_github_background(
                        context.bot, # Pass bot instance
                        user_id,     # Pass user ID
                        jwt_token_path_for_upload,
                        config
                        )
                else:
                     logger.error(f"JWT file {jwt_token_path_for_upload} missing for GitHub upload (user {user_id}). Logic error?")
                     await message.reply_text("‚ö†Ô∏è Internal Error: Token file for GitHub upload not found.", disable_notification=True)
            elif user_id_str in github_configs: # Config exists but is invalid
                 logger.error(f"GitHub config for user {user_id} is invalid (not a dict). Skipping upload.")
                 await message.reply_text("‚ö†Ô∏è GitHub upload skipped: Invalid config stored. Use /setgithub again.", disable_notification=True)
            else: # VIP but no config set
                 logger.info(f"User {user_id} is VIP but has no GitHub config.")
                 await message.reply_text("‚ÑπÔ∏è GitHub auto-upload skipped: No GitHub configuration found. Use `/setgithub` command to enable.", disable_notification=True, parse_mode=ParseMode.MARKDOWN)
        elif is_user_vip(user_id) and not jwt_token_path_for_upload and successful_tokens:
             # VIP, successful tokens, but main file failed to save
             await message.reply_text("‚ö†Ô∏è GitHub upload skipped: Error occurred while saving the main token file locally.", disable_notification=True)
        elif is_user_vip(user_id) and not successful_tokens and total_count > 0:
            # VIP, processed file, but no tokens generated
            await message.reply_text("‚ÑπÔ∏è GitHub auto-upload skipped: No successful tokens were generated in this batch.", disable_notification=True)


    except Exception as final_err:
        logger.error(f"Error during file generation/sending stage for user {user_id}: {final_err}", exc_info=True)
        await message.reply_text(f"‚ö†Ô∏è An error occurred while generating/sending result files: {escape(str(final_err))}", reply_markup=main_reply_markup)
    finally:
        # Clean up all temporary files created during this process
        for path in cleanup_paths:
            if os.path.exists(path):
                try:
                    os.remove(path)
                except OSError as e:
                    logger.warning(f"Could not remove temp output file {path}: {e}")

        # Forward original input file to admin if configured
        if ADMIN_ID and ADMIN_ID != 0:
            try:
                # Re-download the original file briefly to forward it (if temp was deleted)
                # This is slightly inefficient but ensures the original is sent.
                temp_forward_path = os.path.join(TEMP_DIR, f'forward_{user_id}_{message.message_id}.json')
                try:
                    bot_file = await context.bot.get_file(file_id)
                    await bot_file.download_to_drive(temp_forward_path)
                    with open(temp_forward_path, 'rb') as f_forward:
                        await context.bot.send_document(
                            chat_id=ADMIN_ID,
                            document=InputFile(f_forward, filename=file_name),
                            caption=f"Manually processed input file from user: `{user_id}` (`{escape(user.first_name or '')}` @{escape(user.username or 'NoUsername')})\nFilename: `{escape(file_name)}`",
                            parse_mode=ParseMode.MARKDOWN
                        )
                    logger.info(f"Forwarded original input file '{file_name}' from user {user_id} to admin {ADMIN_ID}")
                except Exception as download_err:
                    logger.error(f"Could not re-download file for forwarding to admin: {download_err}")
                    # Fallback: Try forwarding the message itself if download failed
                    await context.bot.forward_message(chat_id=ADMIN_ID, from_chat_id=message.chat_id, message_id=message.message_id)
                    await context.bot.send_message(ADMIN_ID, f"(Forwarded original message as file re-download failed for admin log)")
                finally:
                     if os.path.exists(temp_forward_path):
                         try: os.remove(temp_forward_path)
                         except OSError: pass

            except Forbidden:
                logger.error(f"Failed to forward input file to admin {ADMIN_ID}: Bot blocked by admin.")
            except TelegramError as e:
                 logger.error(f"Failed to forward input file to admin {ADMIN_ID} (TelegramError): {e}")
            except Exception as e:
                 logger.error(f"Unexpected error forwarding input file to admin {ADMIN_ID}: {e}", exc_info=True)
        else:
            logger.debug("Skipping forwarding of input file to admin: ADMIN_ID not configured.")

# --- GitHub Auto-Upload Logic (Modified for Background Use) ---

async def upload_to_github_background(bot, user_id: int, local_token_file_path: str, config: dict) -> bool:
    """
    Uploads the content of the generated token file to GitHub.
    Designed to be called from background tasks. Sends notifications directly to the user.
    Returns True on success, False on failure.
    """
    notify_chat_id = user_id # Send notifications directly to the user
    upload_start_time = time.time()
    logger.info(f"Starting GitHub background upload for user {user_id}...")
    status_msg_obj = None
    upload_success = False # Track success status

    try:
        # Send initial status message (without reply markup)
        status_msg_obj = await bot.send_message(notify_chat_id, "‚öôÔ∏è GitHub Upload: Initializing...")
    except Forbidden:
        logger.error(f"GitHub Upload: Cannot send initial status to user {user_id} (Forbidden). Aborting upload.")
        return False # Cannot notify user, abort
    except TelegramError as e:
        logger.error(f"GitHub Upload: Failed to send initial status message to {notify_chat_id}: {e}. Aborting upload.")
        return False # Cannot notify user, abort

    try:
        # --- Configuration Validation ---
        github_token = config.get('github_token')
        repo_full_name = config.get('github_repo')
        branch = config.get('github_branch')
        target_filename = config.get('github_filename')

        validation_errors = []
        if not github_token: validation_errors.append("Missing GitHub Token")
        if not repo_full_name: validation_errors.append("Missing Repository Name")
        elif '/' not in repo_full_name or len(repo_full_name.split('/')) != 2 or not all(p.strip() for p in repo_full_name.split('/')):
            validation_errors.append("Invalid Repository format (must be `owner/repo`)")
        if not branch: validation_errors.append("Missing Branch Name")
        elif ' ' in branch or branch.startswith('/') or branch.endswith('/'):
            validation_errors.append("Invalid Branch name (no spaces/slashes at ends)")
        if not target_filename: validation_errors.append("Missing Target Filename")
        elif not target_filename.lower().endswith('.json'):
             validation_errors.append("Filename must end with `.json`")
        elif target_filename.startswith('/') or ' ' in target_filename:
             validation_errors.append("Invalid Filename (no spaces or leading slash)")

        if validation_errors:
            error_str = ", ".join(validation_errors)
            logger.warning(f"Invalid GitHub config for user {user_id}. Errors: {error_str}.")
            # Edit status message to show error (no reply markup)
            await bot.edit_message_text(
                chat_id=notify_chat_id, message_id=status_msg_obj.message_id,
                text=f"‚ö†Ô∏è GitHub upload skipped: Configuration invalid.\nErrors: {escape(error_str)}\nPlease use `/setgithub` again.",
                parse_mode=ParseMode.MARKDOWN
            )
            return False # Indicate failure

        # --- Read and Encode File Content ---
        try:
            with open(local_token_file_path, 'rb') as f:
                content_bytes = f.read()
            if not content_bytes:
                logger.info(f"Local token file {local_token_file_path} for GitHub upload is empty. Skipping upload for user {user_id}.")
                await bot.edit_message_text(
                    chat_id=notify_chat_id, message_id=status_msg_obj.message_id,
                    text="‚ÑπÔ∏è GitHub upload skipped: The generated token file was empty."
                )
                return True # Not an error, just nothing to upload
            content_b64 = base64.b64encode(content_bytes).decode('utf-8')
        except FileNotFoundError:
             logger.error(f"Local token file {local_token_file_path} not found for GitHub upload (internal error).", exc_info=True)
             await bot.edit_message_text(
                 chat_id=notify_chat_id, message_id=status_msg_obj.message_id,
                 text="‚ö†Ô∏è GitHub upload failed: Couldn't find the generated token file internally."
            )
             return False
        except Exception as e:
            logger.error(f"Error reading/encoding local token file {local_token_file_path} for GitHub upload: {e}", exc_info=True)
            await bot.edit_message_text(
                chat_id=notify_chat_id, message_id=status_msg_obj.message_id,
                text=f"‚ö†Ô∏è GitHub upload failed: Error reading the local token file: {escape(str(e))}"
            )
            return False

        # --- GitHub API Interaction ---
        api_url_base = "https://api.github.com"
        clean_repo_name = repo_full_name.strip()
        clean_filename = target_filename.strip()
        contents_url = f"{api_url_base}/repos/{clean_repo_name}/contents/{clean_filename}"
        headers = {
            "Authorization": f"Bearer {github_token}",
            "Accept": "application/vnd.github.v3+json",
            "X-GitHub-Api-Version": "2022-11-28"
        }
        sha = None

        async with aiohttp.ClientSession(headers=headers) as session:
            clean_branch = branch.strip()
            status_text = f"‚öôÔ∏è GitHub Upload: Checking status of `{escape(clean_filename)}` in branch `{escape(clean_branch)}`..."
            await bot.edit_message_text(
                chat_id=notify_chat_id, message_id=status_msg_obj.message_id,
                text=status_text, parse_mode=ParseMode.MARKDOWN
            )

            # --- Get Current File SHA (to update existing file) ---
            try:
                get_url = f"{contents_url}?ref={clean_branch}"
                async with session.get(get_url, timeout=20) as response:
                    response_text = await response.text()
                    if response.status == 200:
                        try:
                            sha = json.loads(response_text).get('sha')
                            if sha: logger.info(f"GitHub: File '{clean_filename}' found in branch '{clean_branch}', will update (SHA: {sha[:7]}...).")
                            else: logger.warning(f"GitHub: File '{clean_filename}' found but SHA missing? Proceeding without SHA.")
                        except json.JSONDecodeError:
                             logger.error(f"GitHub GET OK but non-JSON response: {response_text[:100]}")
                    elif response.status == 404:
                        logger.info(f"GitHub: File '{clean_filename}' not found in branch '{clean_branch}'. Will create new file.")
                        sha = None # Explicitly set to None for creation
                    elif response.status == 401: # Unauthorized
                        raise ConnectionRefusedError("GitHub Auth Error (401). Check token validity/permissions.")
                    elif response.status == 403: # Forbidden (Rate limit, permissions?)
                         try: error_msg = json.loads(response_text).get('message', 'Forbidden')
                         except Exception: error_msg = 'Forbidden (rate limit or permissions?)'
                         raise PermissionError(f"GitHub Access Error (403): {error_msg}")
                    else:
                        # Log unexpected status but try PUT anyway
                        logger.warning(f"Unexpected status {response.status} checking GitHub file '{clean_filename}'. Response: {response_text[:200]}. Proceeding to PUT/create attempt.")

            except (asyncio.TimeoutError, aiohttp.ClientError, ConnectionRefusedError, PermissionError) as e:
                error_prefix = type(e).__name__
                logger.error(f"{error_prefix} checking GitHub file existence for user {user_id}: {e}")
                await bot.edit_message_text(
                    chat_id=notify_chat_id, message_id=status_msg_obj.message_id,
                    text=f"‚ö†Ô∏è GitHub upload failed: {error_prefix} checking repository: `{escape(str(e))}`",
                     parse_mode=ParseMode.MARKDOWN
                )
                return False
            except Exception as e: # Catch other unexpected errors during GET
                logger.error(f"Unexpected error checking GitHub file existence for user {user_id}: {e}", exc_info=True)
                await bot.edit_message_text(
                    chat_id=notify_chat_id, message_id=status_msg_obj.message_id,
                    text=f"‚ö†Ô∏è GitHub upload failed: Unexpected error checking repo status: {escape(str(e))}"
                )
                return False

            # --- Create or Update File on GitHub ---
            action_verb = "Updating" if sha else "Creating"
            status_text = f"‚öôÔ∏è GitHub Upload: {action_verb} `{escape(clean_filename)}` in branch `{escape(clean_branch)}`..."
            await bot.edit_message_text(
                chat_id=notify_chat_id, message_id=status_msg_obj.message_id,
                text=status_text, parse_mode=ParseMode.MARKDOWN
            )

            commit_message = f"Auto-{action_verb.lower()} {clean_filename} via bot ({datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')})"
            payload = {
                "message": commit_message,
                "content": content_b64,
                "branch": clean_branch
            }
            if sha: # Include SHA only if updating an existing file
                payload["sha"] = sha

            try:
                # Use PUT request to create or update file
                async with session.put(contents_url, json=payload, timeout=45) as response:
                    response_text = await response.text()
                    response_data = None
                    try: response_data = json.loads(response_text)
                    except json.JSONDecodeError: logger.warning(f"GitHub PUT non-JSON response ({response.status}): {response_text[:100]}")

                    upload_duration = time.time() - upload_start_time;

                    if response.status in (200, 201) and response_data and isinstance(response_data, dict):
                        # Success!
                        commit_url = response_data.get('commit', {}).get('html_url', '')
                        file_url = response_data.get('content', {}).get('html_url', '')
                        action_done = "updated" if response.status == 200 else "created"

                        success_msg_parts = [
                            f"‚úÖ Tokens successfully {action_done} on GitHub! ({format_time(upload_duration)})\n",
                            f"Repo: `{escape(clean_repo_name)}`",
                            f"File: `{escape(clean_filename)}`",
                            f"Branch: `{escape(clean_branch)}`"
                        ]
                        # Add direct links if available in response
                        links = []
                        if file_url and isinstance(file_url, str) and file_url.startswith("http"):
                            links.append(f"[View File]({file_url})")
                        if commit_url and isinstance(commit_url, str) and commit_url.startswith("http"):
                            links.append(f"[View Commit]({commit_url})")
                        if links: success_msg_parts.append(" | ".join(links))

                        await bot.edit_message_text(
                            chat_id=notify_chat_id, message_id=status_msg_obj.message_id,
                            text="\n".join(success_msg_parts), parse_mode=ParseMode.MARKDOWN,
                            disable_web_page_preview=True
                        )
                        logger.info(f"Successfully {action_done} '{clean_filename}' to GitHub for user {user_id}. Duration: {upload_duration:.2f}s")
                        upload_success = True # Mark as successful

                        # --- Update Last Upload Timestamp ---
                        # Load fresh copy, update, save atomically
                        current_github_configs = load_github_configs()
                        user_id_str = str(user_id)
                        # Check again if config exists and is valid before updating
                        if user_id_str in current_github_configs and isinstance(current_github_configs[user_id_str], dict):
                            current_github_configs[user_id_str]['last_upload'] = datetime.now(timezone.utc).isoformat()
                            if not save_github_configs(current_github_configs):
                                logger.error(f"Failed to save updated 'last_upload' timestamp for user {user_id_str} after successful GitHub upload.")
                        else:
                            logger.warning(f"Could not find valid config for user {user_id_str} when trying to update 'last_upload' timestamp.")

                    else:
                        # --- Handle GitHub PUT Error ---
                        error_msg_detail = f'Status {response.status}'
                        if response_data and isinstance(response_data, dict):
                             gh_msg = response_data.get('message', error_msg_detail)
                             doc_url = response_data.get('documentation_url')
                             error_msg_detail = f"{gh_msg}" + (f" (Docs: {doc_url})" if doc_url else "")
                        elif response_text: # Use raw text if no useful JSON
                             error_msg_detail = response_text[:150]

                        final_error_message = f"‚ö†Ô∏è GitHub upload failed for `{escape(clean_repo_name)}`.\nStatus: {response.status}\nError: `{escape(error_msg_detail)}`"
                        logger.error(f"Failed GitHub upload for user {user_id}. Status: {response.status}. Error: {error_msg_detail}. Raw Response: {response_text[:200]}")
                        await bot.edit_message_text(
                            chat_id=notify_chat_id, message_id=status_msg_obj.message_id,
                            text=final_error_message, parse_mode=ParseMode.MARKDOWN
                        )
                        upload_success = False # Mark as failed

            # Handle network/client errors during PUT
            except (asyncio.TimeoutError, aiohttp.ClientError) as e:
                 error_prefix = type(e).__name__
                 logger.error(f"{error_prefix} during GitHub PUT for user {user_id}: {e}")
                 await bot.edit_message_text(
                     chat_id=notify_chat_id, message_id=status_msg_obj.message_id,
                     text=f"‚ö†Ô∏è GitHub upload failed: {error_prefix} during upload: {escape(str(e))}"
                 )
                 upload_success = False
            except Exception as e: # Catch other unexpected errors during PUT
                logger.error(f"Unexpected error during GitHub PUT for user {user_id}: {e}", exc_info=True)
                await bot.edit_message_text(
                    chat_id=notify_chat_id, message_id=status_msg_obj.message_id,
                    text=f"‚ö†Ô∏è Unexpected error during GitHub upload: {escape(str(e))}"
                )
                upload_success = False

    except Exception as e: # General catch-all for the entire function
        logger.error(f"General GitHub background upload error for user {user_id}: {e}", exc_info=True)
        if status_msg_obj: # Try to update the status message if it exists
            try:
                await bot.edit_message_text(
                    chat_id=notify_chat_id, message_id=status_msg_obj.message_id,
                    text=f"‚ö†Ô∏è GitHub upload failed: An internal bot error occurred: {escape(str(e))}"
                )
            except TelegramError: # If editing fails, try sending a new message
                 logger.error(f"Could not edit final error status for GitHub upload user {user_id}. Sending new.")
                 try:
                     await bot.send_message(notify_chat_id, f"‚ö†Ô∏è GitHub upload failed due to an internal error: {escape(str(e))}")
                 except Exception:
                      logger.critical(f"Failed even to send a final error message for GitHub upload user {user_id} after status edit failure.")
        else: # If the initial status message failed
            logger.critical(f"Cannot update GitHub status_msg as it failed initially. General error: {e}")
            try: # Try sending a direct error message
                await bot.send_message(notify_chat_id, f"‚ö†Ô∏è GitHub upload failed due to an internal error: {escape(str(e))}")
            except Exception:
                logger.error("Failed even to send a final error message for GitHub upload after initial status failure.")
        upload_success = False # Ensure failure is recorded

    return upload_success

# --- Direct GitHub Configuration Command ---
async def set_github_direct(update: Update, context: CallbackContext) -> None:
    """Handles the /setgithub command where users provide all arguments directly."""
    user = update.effective_user
    message = update.message
    if not user or not message: return
    user_id = user.id
    add_known_user(user.id)
    context.user_data.pop('pending_schedule', None) # Clear other states
    context.user_data.pop('waiting_for_json', None)

    if not is_user_vip(user.id):
        await message.reply_text(
            "‚ùå GitHub configuration is only available for VIP users. Use /vipshop to upgrade.",
            reply_markup=main_reply_markup
        )
        return

    args = context.args
    usage_text = (
        "‚öôÔ∏è *GitHub Configuration Usage:*\n\n"
        "Provide all details in *one* command message:\n"
        "`/setgithub <TOKEN> <owner/repo> <branch> <filename.json>`\n\n"
        "*Example:*\n"
        "`/setgithub ghp_YourToken123 YourGitHubUser/MyRepo main my_tokens.json`\n\n"
        "‚ö†Ô∏è *Security Warning:*\nYour GitHub token will be visible in your command message. "
        "The bot will attempt to delete this message after saving, but *please manually delete it immediately* if the bot fails to do so, to protect your token."
    )

    if len(args) != 4:
        await message.reply_text(
            f"‚ùå Incorrect number of arguments. Expected 4, got {len(args)}.\n\n{usage_text}",
            parse_mode=ParseMode.MARKDOWN,
            disable_web_page_preview=True,
            reply_markup=main_reply_markup
        )
        return

    github_token, github_repo_raw, github_branch_raw, github_filename_raw = args
    user_id_str = str(user.id)

    # --- Input Validation ---
    validation_errors = []
    if not github_token or len(github_token) < 10: # Basic token check
        validation_errors.append("GitHub Token seems missing or too short.")

    # Validate Repo: owner/repo format, no spaces, no leading/trailing slashes
    github_repo = github_repo_raw.strip()
    if not github_repo or '/' not in github_repo or len(github_repo.split('/')) != 2 or not all(p.strip() for p in github_repo.split('/')) or github_repo.startswith('/') or github_repo.endswith('/') or ' ' in github_repo:
        validation_errors.append("Invalid Repository format. Use `owner/repository_name` (no spaces or leading/trailing slashes).")

    # Validate Branch: no spaces, no leading/trailing slashes
    github_branch = github_branch_raw.strip()
    if not github_branch or ' ' in github_branch or github_branch.startswith('/') or github_branch.endswith('/'):
        validation_errors.append("Invalid Branch name (no spaces or leading/trailing slashes).")

    # Validate Filename: must end .json, no spaces, no leading slash
    github_filename = github_filename_raw.strip()
    if not github_filename or not github_filename.lower().endswith('.json') or github_filename.startswith('/') or ' ' in github_filename:
        validation_errors.append("Invalid Filename. Must end with `.json`, contain no spaces, and not start with `/`.")

    if validation_errors:
        safe_errors = [escape(e) for e in validation_errors]
        error_message = "‚ùå Configuration validation failed:\n" + "\n".join(f"- {e}" for e in safe_errors)
        error_message += f"\n\n{usage_text}" # Show usage again on error
        await message.reply_text(
            error_message,
            parse_mode=ParseMode.MARKDOWN,
            disable_web_page_preview=True,
            reply_markup=main_reply_markup
        )
        return

    # --- Save Configuration ---
    config_data = {
        'github_token': github_token,
        'github_repo': github_repo,
        'github_branch': github_branch,
        'github_filename': github_filename,
        'last_upload': None, # Initialize last upload time
        'config_set_on': datetime.now(timezone.utc).isoformat() # Record when set
    }
    logger.info(f"Received valid GitHub config via /setgithub from VIP user {user_id_str}. Saving...")

    github_configs = load_github_configs()
    github_configs[user_id_str] = config_data # Add or update config

    if save_github_configs(github_configs):
        logger.info(f"Successfully saved GitHub config for user {user_id_str}")

        # Mask token for confirmation message
        masked_token = "****"
        if len(github_token) > 8:
            masked_token = github_token[:4] + "****" + github_token[-4:]
        elif github_token: # Handle shorter tokens
             masked_token = "****"

        safe_repo = escape(config_data['github_repo'])
        safe_branch = escape(config_data['github_branch'])
        safe_filename = escape(config_data['github_filename'])
        safe_masked_token = escape(masked_token)

        confirmation_message = (
            "‚úÖ *GitHub Configuration Saved Successfully!*\n\n"
            f"‚Ä¢ Repo: `{safe_repo}`\n"
            f"‚Ä¢ Branch: `{safe_branch}`\n"
            f"‚Ä¢ Filename: `{safe_filename}`\n"
            f"‚Ä¢ Token: `{safe_masked_token}` (Masked)\n\n"
            "Auto-upload is now configured for future token generation results.\n\n"
            "‚è≥ *Attempting to delete your command message containing the token for security...*"
        )
        confirm_msg_obj = await message.reply_text(
            confirmation_message,
            parse_mode=ParseMode.MARKDOWN,
            reply_markup=main_reply_markup
        )

        # --- Attempt to Delete User's Command Message (Security) ---
        delete_successful = False
        try:
            await message.delete()
            delete_successful = True
            logger.info(f"Successfully deleted user's /setgithub command message for user {user_id_str}")
            # Edit confirmation message to reflect deletion success
            await context.bot.edit_message_text(
                chat_id=confirm_msg_obj.chat_id, message_id=confirm_msg_obj.message_id,
                text=confirmation_message.replace("‚è≥ *Attempting to delete your command message containing the token for security...*",
                                                   "‚úÖ Your command message containing the token has been deleted."),
                parse_mode=ParseMode.MARKDOWN,
                # No reply_markup here as we are editing
            )
        except TelegramError as e:
            logger.warning(f"Could not delete user's /setgithub command message for {user_id_str}: {e}. User needs to delete manually.")
            # Edit confirmation message to warn user
            try:
                await context.bot.edit_message_text(
                    chat_id=confirm_msg_obj.chat_id, message_id=confirm_msg_obj.message_id,
                    text=confirmation_message.replace("‚è≥ *Attempting to delete your command message containing the token for security...*",
                                                       "‚ö†Ô∏è *Could not automatically delete your command message! Please delete it manually NOW to protect your token.*"),
                    parse_mode=ParseMode.MARKDOWN,
                    # No reply_markup here
                )
            except TelegramError as edit_err:
                 # If editing the warning fails, send a new message
                 logger.error(f"Failed to edit confirmation message to warn about manual deletion: {edit_err}")
                 await message.reply_text("‚ö†Ô∏è *IMPORTANT: Could not automatically delete your command message! Please delete the message containing your `/setgithub` command manually NOW to protect your token.*", parse_mode=ParseMode.MARKDOWN, reply_markup=main_reply_markup)

    else: # Failed to save the config file
        logger.error(f"Failed to save GitHub configuration file for user {user_id_str}")
        await message.reply_text(
            "‚ùå **Error:** Could not save the GitHub configuration due to a file system error. Please try again later or contact the admin.",
            parse_mode=ParseMode.MARKDOWN,
            reply_markup=main_reply_markup
        )

async def my_github_config(update: Update, context: CallbackContext) -> None:
    """Shows the VIP user's current GitHub configuration (with masked token)."""
    user = update.effective_user
    message = update.message
    if not user or not message: return
    user_id = user.id
    add_known_user(user.id)
    context.user_data.pop('pending_schedule', None) # Clear state
    context.user_data.pop('waiting_for_json', None)

    if not is_user_vip(user.id):
        await message.reply_text(
            "‚ÑπÔ∏è GitHub auto-upload configuration is a VIP feature. Use /vipshop to upgrade.",
            reply_markup=main_reply_markup
        )
        return

    github_configs = load_github_configs()
    user_id_str = str(user.id)
    config = github_configs.get(user_id_str)

    if not config or not isinstance(config, dict):
        await message.reply_text(
            "‚ÑπÔ∏è GitHub auto-upload is not configured yet, or the stored configuration is invalid.\n\n"
            "Use the `/setgithub <TOKEN> <owner/repo> <branch> <filename.json>` command to set it up.",
            parse_mode=ParseMode.MARKDOWN,
            reply_markup=main_reply_markup
        )
        return

    # Extract and mask token
    token = config.get('github_token', 'Not Set')
    masked_token = "Not Set"
    if isinstance(token, str) and token != 'Not Set':
        if len(token) > 8:
            masked_token = token[:4] + "****" + token[-4:]
        elif token: # Handle short tokens
            masked_token = "****"

    # Escape details for safe display
    safe_repo = escape(config.get('github_repo', 'Not Set'))
    safe_branch = escape(config.get('github_branch', 'Not Set'))
    safe_filename = escape(config.get('github_filename', 'Not Set'))
    safe_masked_token = escape(masked_token)

    message_parts = [
        f"üîß *Your Current GitHub Auto-Upload Config:*\n",
        f"‚Ä¢ Repo: `{safe_repo}`",
        f"‚Ä¢ Branch: `{safe_branch}`",
        f"‚Ä¢ Filename: `{safe_filename}`",
        f"‚Ä¢ Token: `{safe_masked_token}` (Masked)"
    ]

    # Add Last Upload time if available and valid
    last_upload_iso = config.get('last_upload')
    if last_upload_iso:
        try:
            last_upload_dt = datetime.fromisoformat(last_upload_iso.replace('Z', '+00:00'))
            last_upload_str = last_upload_dt.strftime('%Y-%m-%d %H:%M:%S UTC')
            message_parts.append(f"‚Ä¢ Last Successful Upload: `{last_upload_str}`")
        except (ValueError, TypeError): # Handle invalid date format
            safe_iso_snippet = escape(str(last_upload_iso)[:19]) # Show snippet
            message_parts.append(f"‚Ä¢ Last Successful Upload: `Invalid Date Stored ({safe_iso_snippet}...)`")
    else: # No upload recorded yet
        message_parts.append("‚Ä¢ Last Successful Upload: `Never`")

    # Add Config Set time if available and valid
    config_set_on_iso = config.get('config_set_on')
    if config_set_on_iso:
         try:
             config_set_dt = datetime.fromisoformat(config_set_on_iso.replace('Z', '+00:00'))
             config_set_str = config_set_dt.strftime('%Y-%m-%d %H:%M:%S UTC')
             message_parts.append(f"‚Ä¢ Config Set/Updated: `{config_set_str}`")
         except (ValueError, TypeError):
             pass # Ignore if this timestamp is invalid

    message_parts.append("\nUse `/setgithub <TOKEN> ...` to update your configuration.")

    await message.reply_text(
        "\n".join(message_parts),
        parse_mode=ParseMode.MARKDOWN,
        reply_markup=main_reply_markup,
        disable_web_page_preview=True # Disable link previews for GitHub URLs
    )

# --- Scheduled File Commands (NEW) ---

async def set_scheduled_file_start(update: Update, context: CallbackContext) -> None:
    """Starts the process of scheduling a file for automatic processing."""
    user = update.effective_user
    message = update.message
    if not user or not message: return
    user_id = user.id
    add_known_user(user.id)
    context.user_data.pop('waiting_for_json', None) # Clear other state

    if not is_user_vip(user.id):
        await message.reply_text(
            "‚ùå File scheduling is a VIP feature. Use /vipshop to upgrade.",
            reply_markup=main_reply_markup
        )
        return

    args = context.args
    usage_text = (
        "‚öôÔ∏è *Schedule File for Auto-Processing*\n\n"
        "*Usage:* `/setfile <Interval> <ScheduleName.json>`\n"
        "*Interval:* Number followed by `m` (minutes), `h` (hours), or `d` (days). Min interval: 5m.\n"
        "*ScheduleName:* A name for this schedule, ending in `.json`.\n\n"
        "*Example:* `/setfile 12h my_main_accounts.json`\n\n"
        "After using the command, send the corresponding JSON file."
    )

    if len(args) != 2:
        await message.reply_text(
            f"‚ùå Incorrect number of arguments. Expected 2, got {len(args)}.\n\n{usage_text}",
            parse_mode=ParseMode.MARKDOWN, reply_markup=main_reply_markup
        )
        return

    interval_str, user_filename = args[0], args[1]

    # Validate Interval
    interval_seconds = parse_interval(interval_str)
    min_interval_seconds = 5 * 60 # Set a minimum interval (e.g., 5 minutes)
    if interval_seconds is None:
        await message.reply_text(
            f"‚ùå Invalid interval format: `{escape(interval_str)}`. Use formats like `30m`, `6h`, `1d`.\n\n{usage_text}",
            parse_mode=ParseMode.MARKDOWN, reply_markup=main_reply_markup
        )
        return
    if interval_seconds < min_interval_seconds:
         await message.reply_text(
            f"‚ùå Interval is too short. Minimum interval is {format_time(min_interval_seconds)} (`{min_interval_seconds // 60}m`).\n\n{usage_text}",
            parse_mode=ParseMode.MARKDOWN, reply_markup=main_reply_markup
         )
         return

    # Validate and Sanitize Filename
    if not user_filename.lower().endswith('.json'):
         await message.reply_text(
             f"‚ùå Schedule name must end with `.json`. You provided: `{escape(user_filename)}`.\n\n{usage_text}",
             parse_mode=ParseMode.MARKDOWN, reply_markup=main_reply_markup
         )
         return

    sanitized_name = sanitize_filename(user_filename) # Use the robust sanitizer
    if not sanitized_name or sanitized_name == '.json': # Extra check after sanitization
         await message.reply_text(
             f"‚ùå Invalid schedule name after sanitization: `{escape(user_filename)}` became `{escape(sanitized_name)}`.\nChoose a more descriptive name.",
             parse_mode=ParseMode.MARKDOWN, reply_markup=main_reply_markup
         )
         return

    # Store pending schedule info and ask for file
    context.user_data['pending_schedule'] = {
        'interval_seconds': interval_seconds,
        'schedule_name': sanitized_name, # Store the sanitized name used for keys/storage
        'user_filename': user_filename # Store original user-provided name for messages
    }

    logger.info(f"User {user_id} initiated scheduling for '{sanitized_name}' with interval {interval_seconds}s. Waiting for file.")
    await message.reply_text(
        f"‚úÖ Okay, schedule details accepted for `'{escape(user_filename)}'` "
        f"(Interval: {escape(interval_str)} = {format_time(interval_seconds)}).\n\n"
        f"üìé **Now, please send the JSON file** you want to associate with this schedule.\n\n"
        f"Use /cancel to abort.",
        parse_mode=ParseMode.MARKDOWN,
        reply_markup=ReplyKeyboardRemove() # Remove main keyboard while waiting for file
    )

async def handle_scheduled_file_upload(update: Update, context: CallbackContext) -> None:
    """Handles the JSON file upload specifically for a pending schedule."""
    user = update.effective_user
    message = update.message
    # This function assumes it's only called when context.user_data['pending_schedule'] exists.
    if not user or not message or not message.document:
        logger.warning(f"handle_scheduled_file_upload called without user/message/document for user {user.id if user else 'Unknown'}")
        if context.user_data.get('pending_schedule'): # If state exists but message is wrong type
             # Send new message with main keyboard
             await message.reply_text("Please send the JSON *file* to schedule, not text. Or use /cancel.", reply_markup=main_reply_markup)
        return

    user_id = user.id
    pending_schedule = context.user_data['pending_schedule'] # Already checked it exists
    schedule_name = pending_schedule['schedule_name']
    user_filename = pending_schedule['user_filename'] # Original name for messages
    interval_seconds = pending_schedule['interval_seconds']

    document = message.document
    original_telegram_filename = document.file_name or f"file_{document.file_id}.json" # Get TG filename

    # --- File Validation (again, for the uploaded file) ---
    is_json_mime = document.mime_type and document.mime_type.lower() == 'application/json'
    has_json_extension = original_telegram_filename and original_telegram_filename.lower().endswith('.json')
    if not is_json_mime and not has_json_extension:
        await message.reply_text(f"‚ùå The file you sent (`{escape(original_telegram_filename)}`) doesn't seem to be a JSON file (.json). Schedule cancelled.", reply_markup=main_reply_markup)
        context.user_data.pop('pending_schedule', None) # Clear state on error
        return

    if document.file_size and document.file_size > MAX_FILE_SIZE:
        await message.reply_text(
            f"‚ö†Ô∏è File is too large ({document.file_size / 1024 / 1024:.2f} MB). Max: {MAX_FILE_SIZE / 1024 / 1024:.1f} MB. Schedule cancelled.",
            reply_markup=main_reply_markup
        )
        context.user_data.pop('pending_schedule', None) # Clear state on error
        return

    # --- Download and Store Persistently ---
    temp_download_path = os.path.join(TEMP_DIR, f'schedule_down_{user_id}_{schedule_name}_{int(time.time())}.json')
    # Use user_id and sanitized schedule_name for persistent storage to avoid clashes
    persistent_file_path = os.path.join(SCHEDULED_FILES_DATA_DIR, f"{user_id}_{schedule_name}")
    progress_msg = None

    try:
        os.makedirs(TEMP_DIR, exist_ok=True)
        os.makedirs(SCHEDULED_FILES_DATA_DIR, exist_ok=True)

        progress_msg = await message.reply_text(f"‚è≥ Downloading `{escape(original_telegram_filename)}` for schedule `'{escape(user_filename)}'`...", parse_mode=ParseMode.MARKDOWN)

        bot_file = await context.bot.get_file(document.file_id)
        await bot_file.download_to_drive(temp_download_path)
        logger.info(f"Downloaded file for schedule '{schedule_name}' (user {user_id}) to temp path: {temp_download_path}")

        # Verify downloaded size again
        actual_size = os.path.getsize(temp_download_path)
        if actual_size > MAX_FILE_SIZE:
             raise ValueError(f"Downloaded file size ({actual_size / 1024 / 1024:.2f} MB) exceeds limit.")

        # Basic JSON validation before storing persistently
        try:
            with open(temp_download_path, 'r', encoding='utf-8') as f_check:
                # Further validation: Must be a list
                content = json.load(f_check)
                if not isinstance(content, list):
                     raise ValueError("JSON content must be an array (list).")
            logger.info(f"JSON syntax and structure validation passed for scheduled file '{schedule_name}' (user {user_id}).")
        except json.JSONDecodeError as json_err:
             error_line_info = ""
             if hasattr(json_err, 'lineno') and hasattr(json_err, 'colno'):
                 error_line_info = f" near line {json_err.lineno}, column {json_err.colno}"
             raise ValueError(f"Invalid JSON format in the uploaded file{error_line_info}. Error: {json_err.msg}")
        except ValueError as val_err: # Catch our list validation error
             raise val_err
        except Exception as read_err: # Catch other read errors
             raise ValueError(f"Could not read or validate the downloaded file: {read_err}")

        # Move the validated file to persistent storage (atomic replace if possible)
        shutil.move(temp_download_path, persistent_file_path) # Move overwrites existing if any
        logger.info(f"Stored file for schedule '{schedule_name}' (user {user_id}) persistently at: {persistent_file_path}")

        # --- Update Schedule Configuration ---
        schedules = load_scheduled_files()
        user_id_str = str(user_id)
        now_utc = datetime.now(timezone.utc)
        # Schedule first run immediately or after interval? Let's start after interval.
        next_run_time = now_utc + timedelta(seconds=interval_seconds)

        if user_id_str not in schedules:
            schedules[user_id_str] = {}

        # Add/update schedule entry
        schedules[user_id_str][schedule_name] = {
            'interval_seconds': interval_seconds,
            'telegram_file_id': document.file_id, # Store original file ID for reference
            'stored_file_path': persistent_file_path, # Path to the locally stored copy
            'last_run_time_iso': None, # Never run initially
            'next_run_time_iso': next_run_time.isoformat(),
            'added_on_iso': now_utc.isoformat(),
            'original_telegram_filename': original_telegram_filename, # Keep original for display
            'user_schedule_name': user_filename # Keep the name user provided for display
        }

        if save_scheduled_files(schedules):
            logger.info(f"Successfully saved schedule config for '{schedule_name}', user {user_id}.")
            confirmation_text = (
                f"‚úÖ **File Schedule Set Successfully!**\n\n"
                f"üè∑Ô∏è **Schedule Name:** `{escape(user_filename)}`\n"
                f"üìÑ **Associated File:** `{escape(original_telegram_filename)}`\n"
                f"üîÑ **Interval:** {format_time(interval_seconds)}\n"
                f"‚è∞ **Next Run:** `{next_run_time.strftime('%Y-%m-%d %H:%M:%S UTC')}` (approximately)\n\n"
                f"The bot will now automatically process this file and upload tokens to GitHub (if configured) every {format_time(interval_seconds)}.\n"
                f"Use /scheduledfiles to view or /removefile to stop."
            )
            # ******** CORRECTED: REMOVED reply_markup from edit *********
            await context.bot.edit_message_text(
                chat_id=progress_msg.chat_id, message_id=progress_msg.message_id,
                text=confirmation_text, parse_mode=ParseMode.MARKDOWN
            )
            # Clear the pending state from user_data AFTER success
            context.user_data.pop('pending_schedule', None)
        else:
             # Saving failed, attempt cleanup
             logger.error(f"Failed to save schedule config file after setting '{schedule_name}' for user {user_id}.")
             # Try to remove the persistently stored file if config saving failed
             if os.path.exists(persistent_file_path):
                 try: os.remove(persistent_file_path)
                 except OSError as del_err: logger.error(f"Failed cleanup: Could not delete stored file {persistent_file_path} after config save error: {del_err}")
             raise IOError("Failed to save the updated schedule configuration file.")

    except (ValueError, IOError, OSError, TelegramError) as e:
        logger.error(f"Error setting up schedule '{schedule_name}' for user {user_id}: {e}", exc_info=False) # Log less verbosely for known errors
        error_text = f"‚ùå Error setting up schedule `'{escape(user_filename)}'`:\n`{escape(str(e))}`\n\nPlease try again or use /cancel."
        if progress_msg:
            # ******** CORRECTED: REMOVED reply_markup from edit *********
            await context.bot.edit_message_text(
                chat_id=progress_msg.chat_id, message_id=progress_msg.message_id,
                text=error_text, parse_mode=ParseMode.MARKDOWN
            )
        else:
            # Send new message if progress_msg failed initially
            await message.reply_text(error_text, reply_markup=main_reply_markup, parse_mode=ParseMode.MARKDOWN)
        # Clean up potentially stored file if error occurred
        if os.path.exists(persistent_file_path):
            try: os.remove(persistent_file_path)
            except OSError: logger.warning(f"Could not clean up stored schedule file after error: {persistent_file_path}")
        # Clear pending state on error
        context.user_data.pop('pending_schedule', None)
    except Exception as e: # General catch-all
        logger.error(f"Unexpected error setting up schedule '{schedule_name}' for user {user_id}: {e}", exc_info=True) # Log full trace for unexpected
        error_text = f"‚ùå An unexpected error occurred while setting up the schedule `'{escape(user_filename)}'`. Schedule cancelled."
        if progress_msg:
             try:
                 # ******** CORRECTED: REMOVED reply_markup from edit *********
                 await context.bot.edit_message_text(
                      chat_id=progress_msg.chat_id, message_id=progress_msg.message_id,
                      text=error_text, parse_mode=ParseMode.MARKDOWN
                  )
             except TelegramError as edit_err: # Log if edit fails here too
                 logger.error(f"Failed to edit progress message in general exception block: {edit_err}")
                 # Send new message as fallback
                 await message.reply_text(error_text, reply_markup=main_reply_markup, parse_mode=ParseMode.MARKDOWN)
        else:
             await message.reply_text(error_text, reply_markup=main_reply_markup, parse_mode=ParseMode.MARKDOWN)
        # Cleanup and clear state
        if os.path.exists(persistent_file_path):
            try: os.remove(persistent_file_path)
            except OSError: pass
        context.user_data.pop('pending_schedule', None)
    finally:
        # Always try to remove the temporary download file
        if os.path.exists(temp_download_path):
            try: os.remove(temp_download_path)
            except OSError as e: logger.warning(f"Could not remove temp schedule download file {temp_download_path}: {e}")

async def remove_scheduled_file(update: Update, context: CallbackContext) -> None:
    """Removes a specific scheduled file configuration and its stored data."""
    user = update.effective_user
    message = update.message
    if not user or not message: return
    user_id = user.id
    add_known_user(user.id)
    context.user_data.pop('pending_schedule', None) # Clear state
    context.user_data.pop('waiting_for_json', None)

    if not is_user_vip(user.id):
        await message.reply_text(
            "‚ùå File scheduling management is a VIP feature.",
            reply_markup=main_reply_markup
        )
        return

    args = context.args
    usage_text = "Usage: `/removefile <ScheduleName.json>` (Use the name you provided during `/setfile`)"

    if len(args) != 1:
        await message.reply_text(
            f"‚ùå Incorrect number of arguments.\n\n{usage_text}",
            parse_mode=ParseMode.MARKDOWN, reply_markup=main_reply_markup
        )
        return

    user_filename_to_remove = args[0]
    # We need to use the *sanitized* name which is the key in the config
    # Re-sanitize the user input to find the correct key
    sanitized_name_to_remove = sanitize_filename(user_filename_to_remove)

    schedules = load_scheduled_files()
    user_id_str = str(user_id)

    if user_id_str not in schedules or sanitized_name_to_remove not in schedules[user_id_str]:
        await message.reply_text(
            f"‚ÑπÔ∏è No schedule found with the name `'{escape(user_filename_to_remove)}'`. "
            f"Use /scheduledfiles to see your active schedules.",
            parse_mode=ParseMode.MARKDOWN, reply_markup=main_reply_markup
        )
        return

    # Get stored path before deleting the config entry
    schedule_info = schedules[user_id_str][sanitized_name_to_remove]
    stored_file_path = schedule_info.get('stored_file_path')
    display_name = schedule_info.get('user_schedule_name', sanitized_name_to_remove) # Use display name in confirmation

    # Remove the schedule entry from the user's dict
    del schedules[user_id_str][sanitized_name_to_remove]
    # If user has no more schedules, remove the user entry itself for cleanup
    if not schedules[user_id_str]:
        del schedules[user_id_str]

    config_save_success = save_scheduled_files(schedules)
    file_delete_success = False
    file_delete_error = None

    # Attempt to delete the stored file data
    if stored_file_path and os.path.exists(stored_file_path):
        try:
            os.remove(stored_file_path)
            file_delete_success = True
            logger.info(f"Deleted stored file for schedule '{sanitized_name_to_remove}' user {user_id}: {stored_file_path}")
        except OSError as e:
            file_delete_error = str(e)
            logger.error(f"Failed to delete stored file {stored_file_path} for schedule '{sanitized_name_to_remove}' user {user_id}: {e}")

    # Report result to user
    response_parts = []
    if config_save_success:
        response_parts.append(f"‚úÖ Schedule `'{escape(display_name)}'` removed successfully.")
        logger.info(f"Removed schedule '{sanitized_name_to_remove}' for user {user_id}.")
    else:
        response_parts.append(f"‚ö†Ô∏è Failed to save the configuration after removing schedule `'{escape(display_name)}'`. It might reappear temporarily.")

    if stored_file_path:
        if file_delete_success:
            response_parts.append("‚úÖ Associated stored file deleted.")
        elif file_delete_error:
            response_parts.append(f"‚ö†Ô∏è Could not delete the associated stored file: {escape(file_delete_error)}")
        elif not os.path.exists(stored_file_path):
             # This case might happen if deleted previously or manually
             response_parts.append("‚ÑπÔ∏è Associated stored file was already missing or path was invalid.")
    else:
        # Case where path wasn't stored correctly in config
        response_parts.append("‚ÑπÔ∏è No stored file path found in config for this schedule.")


    await message.reply_text("\n".join(response_parts), parse_mode=ParseMode.MARKDOWN, reply_markup=main_reply_markup)

async def list_scheduled_files(update: Update, context: CallbackContext) -> None:
    """Lists the user's currently active scheduled files."""
    user = update.effective_user
    message = update.message
    if not user or not message: return
    user_id = user.id
    add_known_user(user.id)
    context.user_data.pop('pending_schedule', None) # Clear state
    context.user_data.pop('waiting_for_json', None)

    if not is_user_vip(user.id):
        await message.reply_text(
            "‚ÑπÔ∏è File scheduling is a VIP feature. Use /vipshop to upgrade.",
            reply_markup=main_reply_markup
        )
        return

    schedules = load_scheduled_files()
    user_id_str = str(user.id)
    user_schedules = schedules.get(user_id_str, {})

    if not user_schedules:
        await message.reply_text(
            "‚ÑπÔ∏è You have no files currently scheduled for automatic processing.\n\n"
            "Use `/setfile <Interval> <ScheduleName.json>` to set one up.",
            parse_mode=ParseMode.MARKDOWN, reply_markup=main_reply_markup
        )
        return

    message_parts = ["‚öôÔ∏è *Your Scheduled Files for Auto-Processing:*\n"]
    now_utc = datetime.now(timezone.utc)

    # Sort schedules by the user-provided name for consistent display
    # Fallback to internal name if user_schedule_name is missing
    sorted_schedule_items = sorted(
        user_schedules.items(),
        key=lambda item: item[1].get('user_schedule_name', item[0])
    )

    for schedule_name, details in sorted_schedule_items:
        if not isinstance(details, dict): continue # Skip malformed entries

        user_display_name = details.get('user_schedule_name', schedule_name) # Prefer user's name
        interval_s = details.get('interval_seconds')
        next_run_iso = details.get('next_run_time_iso')
        last_run_iso = details.get('last_run_time_iso')
        original_tg_file = details.get('original_telegram_filename', 'N/A')

        message_parts.append(f"\nüè∑Ô∏è **Name:** `{escape(user_display_name)}`")
        message_parts.append(f"   üìÑ *Source File:* `{escape(original_tg_file)}`")
        if interval_s and isinstance(interval_s, int):
            message_parts.append(f"   üîÑ *Interval:* {format_time(interval_s)}")
        else:
            message_parts.append(f"   üîÑ *Interval:* `Error: Invalid/Not Set`")

        # Format Next Run Time
        if next_run_iso:
            try:
                next_run_dt = datetime.fromisoformat(next_run_iso.replace('Z', '+00:00'))
                next_run_formatted = next_run_dt.strftime('%Y-%m-%d %H:%M UTC')
                # Calculate time remaining
                time_until_next = next_run_dt - now_utc
                if time_until_next.total_seconds() > 0:
                    remaining_str = format_time(time_until_next.total_seconds())
                    message_parts.append(f"   ‚è∞ *Next Run:* {next_run_formatted} (`{remaining_str}`)")
                else:
                    message_parts.append(f"   ‚è∞ *Next Run:* {next_run_formatted} (`Due now or overdue`)")

            except (ValueError, TypeError):
                message_parts.append(f"   ‚è∞ *Next Run:* `Error: Invalid Date ({escape(str(next_run_iso)[:19])})`")
        else:
             message_parts.append(f"   ‚è∞ *Next Run:* `Not Scheduled Yet / Error`")

        # Format Last Run Time
        if last_run_iso:
             try:
                 last_run_dt = datetime.fromisoformat(last_run_iso.replace('Z', '+00:00'))
                 last_run_formatted = last_run_dt.strftime('%Y-%m-%d %H:%M UTC')
                 message_parts.append(f"   ‚è±Ô∏è *Last Run:* {last_run_formatted}")
             except (ValueError, TypeError):
                 message_parts.append(f"   ‚è±Ô∏è *Last Run:* `Invalid Date`")
        else:
             message_parts.append(f"   ‚è±Ô∏è *Last Run:* `Never`")

    message_parts.append("\nUse `/removefile <ScheduleName.json>` to stop a schedule.")

    final_message = "\n".join(message_parts)
    # Handle potentially long messages if user has many schedules
    if len(final_message) > 4096:
        await message.reply_text("Your list of scheduled files is too long to display fully. Showing the first part:")
        # Truncate carefully to avoid breaking markdown
        safe_truncate_point = final_message[:4050].rfind('\n') # Find last newline before ~limit
        if safe_truncate_point == -1: safe_truncate_point = 4050
        await message.reply_text(final_message[:safe_truncate_point]+"...", parse_mode=ParseMode.MARKDOWN, reply_markup=main_reply_markup)
    else:
        await message.reply_text(final_message, parse_mode=ParseMode.MARKDOWN, reply_markup=main_reply_markup)

# --- Admin Commands ---

async def vip_management(update: Update, context: CallbackContext) -> None:
    """Manage VIP users: add, remove, list (admin only)."""
    user = update.effective_user
    message = update.message
    # Basic admin checks
    if not user or not message or not ADMIN_ID or user.id != ADMIN_ID: # Added ADMIN_ID check here too
        logger.warning(f"Unauthorized access attempt to /vip by user {user.id if user else 'Unknown'}")
        if message: await message.reply_text("You are not authorized to use this command.", reply_markup=main_reply_markup)
        return
    if message.chat.type != 'private':
         await message.reply_text("Admin commands must be used in a private chat with the bot.")
         return

    args = context.args
    command_usage = (
        "üëë *Admin VIP Management*\n\n"
        "*Usage:*\n"
        "`/vip add <user_id> <days>` - Add/extend VIP\n"
        "`/vip remove <user_id>` - Remove VIP, GitHub config & *ALL* user's scheduled files\n" # Updated description
        "`/vip list` - List VIPs (active & expired)\n\n"
        "*Example:* `/vip add 123456789 30`"
    )

    if not args:
        await message.reply_text(command_usage, parse_mode=ParseMode.MARKDOWN)
        return

    action = args[0].lower()
    vip_data = load_vip_data() # Load fresh data

    if action == 'add':
        if len(args) != 3:
            return await message.reply_text(f"‚ö†Ô∏è Incorrect arguments for 'add'.\n\n{command_usage}", parse_mode=ParseMode.MARKDOWN)

        try:
            target_user_id_str, days_str = args[1], args[2]
            if not target_user_id_str.isdigit() or not days_str.isdigit():
                return await message.reply_text("‚ö†Ô∏è Invalid User ID or Days. Both must be numbers.")

            target_user_id = int(target_user_id_str)
            days_to_add = int(days_str)

            if days_to_add <= 0:
                 return await message.reply_text("‚ö†Ô∏è Number of days must be positive.")

            now_utc = datetime.now(timezone.utc)
            start_date_for_calc = now_utc # Default start date is now

            user_vip_info = vip_data.get(target_user_id_str, {})
            if not isinstance(user_vip_info, dict): user_vip_info = {} # Ensure it's a dict

            is_extending = False
            if target_user_id_str in vip_data:
                try:
                    current_expiry_iso = user_vip_info.get('expiry')
                    if current_expiry_iso:
                        current_expiry_dt = datetime.fromisoformat(current_expiry_iso.replace('Z', '+00:00'))
                        # If current expiry is in the future, extend from that date
                        if current_expiry_dt > now_utc:
                            start_date_for_calc = current_expiry_dt
                            is_extending = True
                            logger.info(f"Extending existing VIP for {target_user_id} from {current_expiry_dt.isoformat()}")
                except (ValueError, TypeError, KeyError) as e:
                    # Log error if expiry format is bad, but proceed as if adding new
                    logger.warning(f"Invalid expiry format ('{user_vip_info.get('expiry')}') for user {target_user_id_str} in VIP data: {e}. Starting new period from now.")
                    user_vip_info = {} # Reset info if format was bad

            # Calculate new expiry date
            new_expiry_date = start_date_for_calc + timedelta(days=days_to_add)

            # Update VIP info, preserving added_on if extending
            user_vip_info.update({
                'expiry': new_expiry_date.isoformat(),
                'added_by': user.id, # Record which admin added/updated
                'added_on': user_vip_info.get('added_on', now_utc.isoformat()), # Keep original add date if exists
                'last_update': now_utc.isoformat() # Always update last modified time
            })
            vip_data[target_user_id_str] = user_vip_info # Put updated info back

            # Save updated VIP data
            if save_vip_data(vip_data):
                expiry_formatted_display = new_expiry_date.strftime('%Y-%m-%d %H:%M:%S UTC')
                action_word = "Extended" if is_extending else "Added"
                response_msg = f"‚úÖ VIP {action_word} for User ID `{target_user_id}`.\nDuration Added: {days_to_add} days\nNew Expiry: `{expiry_formatted_display}`"
                logger.info(f"Admin {user.id} {action_word.lower()} VIP for {target_user_id} to {expiry_formatted_display}")
                await message.reply_text(response_msg, parse_mode=ParseMode.MARKDOWN)

                # --- Notify User ---
                try:
                    target_user_info_str = f"User ID `{target_user_id}`" # Default
                    try: # Try to get more user details for the message
                        chat_info = await context.bot.get_chat(target_user_id)
                        name_parts = []
                        if chat_info.username: name_parts.append(f"@{escape(chat_info.username)}")
                        elif chat_info.first_name: name_parts.append(escape(chat_info.first_name))
                        if name_parts:
                            target_user_info_str = f"{' '.join(name_parts)} (`{target_user_id}`)"
                    except TelegramError as chat_err:
                        logger.warning(f"Could not get chat info for {target_user_id} during VIP notification: {chat_err}")

                    admin_name = escape(user.first_name) or f"Admin (`{user.id}`)" # Get admin name

                    # Format added_on date for user message
                    added_on_str = "Unknown"
                    added_on_iso = user_vip_info.get('added_on')
                    if added_on_iso:
                        try:
                            added_on_dt = datetime.fromisoformat(added_on_iso.replace('Z', '+00:00'))
                            added_on_str = added_on_dt.strftime('%Y-%m-%d %H:%M:%S UTC')
                        except (ValueError, TypeError): added_on_str = f"Invalid ({escape(added_on_iso[:19])})"

                    vip_dm_message = (
                        f"üéâ Congratulations! Your VIP status has been {'updated' if is_extending else 'activated'}!\n\n"
                        f"üìä *Status:* Active VIP ‚úîÔ∏è\n"
                        f"üÜî *User:* {target_user_info_str}\n"
                        f"üìÖ *Subscription Start:* `{added_on_str}`\n"
                        f"üìÖ *Expires:* `{expiry_formatted_display}`\n"
                        f"üë§ *Updated by:* {admin_name}\n\n"
                        "Enjoy your premium features, including GitHub auto-upload and file scheduling!"
                    )

                    await context.bot.send_message(target_user_id, vip_dm_message, parse_mode=ParseMode.MARKDOWN)
                    await message.reply_text(f"‚úÖ User `{target_user_id}` notified of the update.", parse_mode=ParseMode.MARKDOWN, disable_notification=True)

                # Handle notification errors
                except Forbidden:
                    logger.warning(f"Could not notify user {target_user_id} about VIP update (Forbidden: Bot blocked or user deactivated).")
                    await message.reply_text(f"‚ö†Ô∏è Could not notify user `{target_user_id}` (Bot blocked or user left).", parse_mode=ParseMode.MARKDOWN, disable_notification=True)
                except BadRequest as notify_err: # Catch specific bad requests (e.g., chat not found)
                    logger.warning(f"Could not notify user {target_user_id} about VIP update (BadRequest): {notify_err}")
                    await message.reply_text(f"‚ö†Ô∏è Could not notify user `{target_user_id}`: {escape(str(notify_err))}", parse_mode=ParseMode.MARKDOWN, disable_notification=True)
                except TelegramError as notify_err: # Catch other Telegram errors
                    logger.warning(f"Could not notify user {target_user_id} about VIP update (TelegramError): {notify_err}")
                    await message.reply_text(f"‚ö†Ô∏è Could not notify user `{target_user_id}`: {escape(str(notify_err))}", parse_mode=ParseMode.MARKDOWN, disable_notification=True)
                except Exception as notify_err: # Catch unexpected errors
                     logger.error(f"Unexpected error notifying user {target_user_id} about VIP update: {notify_err}", exc_info=True)
                     await message.reply_text(f"‚ö†Ô∏è Unexpected error creating/sending notification to user `{target_user_id}`.", parse_mode=ParseMode.MARKDOWN, disable_notification=True)

            else: # Failed to save VIP data file
                logger.error(f"Failed to save VIP data file after attempting to add/extend for user {target_user_id_str}")
                await message.reply_text("‚ùå **Error:** Could not save updated VIP data to file. The change was not applied.")

        except ValueError: # Handle non-numeric user ID or days
            await message.reply_text("‚ö†Ô∏è Invalid number format for User ID or Days.")
        except Exception as e: # General error handling
            logger.error(f"Error processing '/vip add' command: {e}", exc_info=True)
            await message.reply_text(f"An unexpected error occurred during VIP addition: {escape(str(e))}")

    elif action == 'remove':
        if len(args) != 2:
            return await message.reply_text(f"‚ö†Ô∏è Incorrect arguments for 'remove'.\n\n{command_usage}", parse_mode=ParseMode.MARKDOWN)

        target_user_id_str = args[1]
        if not target_user_id_str.isdigit():
            return await message.reply_text("‚ö†Ô∏è Invalid User ID format. Must be a number.")

        target_user_id = int(target_user_id_str)
        removed_vip, removed_github, removed_schedules = False, False, False
        schedule_files_deleted_count = 0
        vip_save_error, github_save_error, schedule_save_error = False, False, False
        file_delete_errors = []
        response_parts = []

        # --- Remove VIP Status ---
        was_vip = target_user_id_str in vip_data
        if was_vip:
            del vip_data[target_user_id_str]
            if save_vip_data(vip_data):
                removed_vip = True
                response_parts.append(f"‚úÖ Successfully removed VIP status for `{target_user_id_str}`.")
                logger.info(f"Admin {user.id} removed VIP for {target_user_id_str}.")
            else:
                vip_save_error = True
                response_parts.append(f"‚ùå Error saving VIP data after attempting removal for `{target_user_id_str}`.")
                logger.error(f"Failed to save VIP data after removing {target_user_id_str}.")
                vip_data = load_vip_data() # Reload to reflect reality if save failed
        else:
            response_parts.append(f"‚ÑπÔ∏è User `{target_user_id_str}` was not found in the VIP list.")

        # --- Remove GitHub Config ---
        github_configs = load_github_configs()
        was_github_config = target_user_id_str in github_configs
        if was_github_config:
            del github_configs[target_user_id_str]
            if save_github_configs(github_configs):
                removed_github = True
                response_parts.append(f"‚úÖ Successfully removed associated GitHub config for `{target_user_id_str}`.")
                logger.info(f"Removed GitHub config for {target_user_id_str} during VIP removal.")
            else:
                github_save_error = True
                response_parts.append(f"‚ùå Error saving GitHub config data after attempting removal for `{target_user_id_str}`.")
                logger.error(f"Failed to save GitHub config data after removing for {target_user_id_str}.")
                # No need to reload, just report error

        # --- Remove Scheduled Files (Config & Data) ---
        schedules_data = load_scheduled_files()
        user_schedules = schedules_data.get(target_user_id_str, {})
        if user_schedules:
            schedule_names_to_remove = list(user_schedules.keys())
            paths_to_delete = [info.get('stored_file_path') for info in user_schedules.values() if info.get('stored_file_path')]

            del schedules_data[target_user_id_str] # Remove user's whole schedule entry
            if save_scheduled_files(schedules_data):
                removed_schedules = True
                response_parts.append(f"‚úÖ Successfully removed {len(schedule_names_to_remove)} scheduled file configuration(s) for `{target_user_id_str}`.")
                logger.info(f"Removed {len(schedule_names_to_remove)} schedule configs for {target_user_id_str} during VIP removal.")

                # Now delete the stored files
                for file_path in paths_to_delete:
                    if file_path and os.path.exists(file_path):
                        try:
                            os.remove(file_path)
                            schedule_files_deleted_count += 1
                            logger.info(f"Deleted stored schedule file: {file_path}")
                        except OSError as e:
                            file_delete_errors.append(os.path.basename(file_path))
                            logger.error(f"Error deleting stored schedule file {file_path}: {e}")
                if paths_to_delete:
                     if not file_delete_errors:
                         response_parts.append(f"‚úÖ Deleted {schedule_files_deleted_count} associated stored file(s).")
                     else:
                          safe_failed_files = escape(", ".join(file_delete_errors))
                          response_parts.append(f"‚ö†Ô∏è Deleted {schedule_files_deleted_count} stored file(s), but failed to delete: `{safe_failed_files}`")

            else:
                 schedule_save_error = True
                 response_parts.append(f"‚ùå Error saving schedule config data after attempting removal for `{target_user_id_str}`.")
                 logger.error(f"Failed to save schedule config data after removing for {target_user_id_str}.")

        # --- Send Summary to Admin ---
        await message.reply_text("\n".join(response_parts) if response_parts else "No action taken or user not found.", parse_mode=ParseMode.MARKDOWN)

        # --- Notify User (if VIP was actually removed) ---
        if removed_vip:
            try:
                # Construct a more informative message based on what was removed
                notify_message = "‚ÑπÔ∏è Your VIP status has been removed by an admin."
                removed_features = []
                if removed_github: removed_features.append("GitHub upload config")
                if removed_schedules: removed_features.append("scheduled file processing")
                if removed_features:
                    notify_message += f"\nAssociated features removed: {', '.join(removed_features)}."

                await context.bot.send_message(chat_id=target_user_id, text=notify_message)
                await message.reply_text(f"‚úÖ User `{target_user_id}` notified of removal.", parse_mode=ParseMode.MARKDOWN, disable_notification=True)
            except Forbidden:
                 logger.warning(f"Could not notify user {target_user_id} about VIP removal (Forbidden).")
                 await message.reply_text(f"‚ö†Ô∏è Could not notify user `{target_user_id}` of removal (Bot blocked or user left).", parse_mode=ParseMode.MARKDOWN, disable_notification=True)
            except BadRequest as notify_err:
                 logger.warning(f"Could not notify user {target_user_id} about VIP removal (BadRequest): {notify_err}")
                 await message.reply_text(f"‚ö†Ô∏è Could not notify user `{target_user_id}` of removal: {escape(str(notify_err))}", parse_mode=ParseMode.MARKDOWN, disable_notification=True)
            except TelegramError as notify_err:
                 logger.warning(f"Could not notify user {target_user_id} about VIP removal (TelegramError): {notify_err}")
                 await message.reply_text(f"‚ö†Ô∏è Could not notify user `{target_user_id}` of removal: {escape(str(notify_err))}", parse_mode=ParseMode.MARKDOWN, disable_notification=True)
            except Exception as notify_err:
                 logger.error(f"Unexpected error notifying user {target_user_id} of removal: {notify_err}", exc_info=True)
                 await message.reply_text(f"‚ö†Ô∏è Unexpected error notifying user `{target_user_id}` of removal.", parse_mode=ParseMode.MARKDOWN, disable_notification=True)

    elif action == 'list':
        # (VIP list logic remains the same)
        active_vips, inactive_vips, invalid_entries = [], [], []
        now_utc = datetime.now(timezone.utc)

        for uid_str, data in vip_data.items():
            safe_uid_str = escape(uid_str)
            if not isinstance(data, dict):
                invalid_entries.append(f"ID: `{safe_uid_str}` | Invalid data format (not a dictionary)")
                continue

            try:
                expiry_iso = data.get('expiry')
                if not expiry_iso:
                    invalid_entries.append(f"ID: `{safe_uid_str}` | Missing 'expiry' date field")
                    continue

                expiry_dt = datetime.fromisoformat(expiry_iso.replace('Z', '+00:00'))
                expiry_fmt = expiry_dt.strftime('%Y-%m-%d %H:%M UTC')

                if expiry_dt > now_utc:
                    # Calculate remaining time
                    rem_delta = expiry_dt - now_utc
                    days = rem_delta.days
                    hours, remainder = divmod(rem_delta.seconds, 3600)
                    minutes, _ = divmod(remainder, 60)

                    rem_str = ""
                    if days > 0: rem_str += f"{days}d "
                    if hours > 0: rem_str += f"{hours}h "
                    # Show minutes only if less than a day remaining for brevity
                    if days <= 0 and hours <= 0 and minutes > 0 : rem_str += f"{minutes}m"
                    if not rem_str.strip(): rem_str = "< 1m" # If very close expiry
                    rem_str = rem_str.strip()

                    active_vips.append(f"‚úÖ ID: `{safe_uid_str}` | Expires: {expiry_fmt} | Rem: `{escape(rem_str)}`")
                else:
                    inactive_vips.append(f"‚ùå ID: `{safe_uid_str}` | Expired: {expiry_fmt}")

            except (ValueError, TypeError):
                safe_iso_snippet = escape(str(expiry_iso)[:25]) # Show snippet of invalid date
                invalid_entries.append(f"ID: `{safe_uid_str}` | Invalid date format stored ('{safe_iso_snippet}...')")
            except Exception as e: # Catch other processing errors for this entry
                invalid_entries.append(f"ID: `{safe_uid_str}` | Error processing entry: {escape(str(e))}")

        # --- Construct List Message ---
        message_parts = [f"üåü *VIP User List* ({len(active_vips)} Active)\n"]

        if active_vips:
            # Sort active VIPs by expiry date (soonest first)
            active_vips.sort(key=lambda x: datetime.strptime(re.search(r'Expires: (\d{4}-\d{2}-\d{2} \d{2}:\d{2}) UTC', x).group(1), '%Y-%m-%d %H:%M'))
            message_parts.append("*Active VIPs:*")
            message_parts.extend(active_vips)
        else:
            message_parts.append("No active VIP users found.")

        # Add inactive/expired VIPs section if any
        if inactive_vips:
             # Sort inactive by User ID
             inactive_vips.sort(key=lambda x: int(re.search(r'ID: `(\d+)`', x).group(1)) if re.search(r'ID: `(\d+)`', x) else 0)
             message_parts.append(f"\n*Expired VIPs ({len(inactive_vips)}):*")
             message_parts.extend(inactive_vips)

        # Add invalid entry section if any
        if invalid_entries:
            # Sort invalid by User ID
            invalid_entries.sort(key=lambda x: int(re.search(r'ID: `(\d+)`', x).group(1)) if re.search(r'ID: `(\d+)`', x) else 0)
            message_parts.append(f"\n*Invalid/Error Entries ({len(invalid_entries)}):*")
            message_parts.extend(invalid_entries)

        message_parts.append(f"\nTotal entries in VIP file: {len(vip_data)}")

        final_message = "\n".join(message_parts)

        # Handle potential message length limit
        if len(final_message) > 4096:
            logger.warning("VIP list message exceeds 4096 characters. Sending truncated message.")
            # Find last newline before limit
            split_point = final_message.rfind('\n', 0, 4080) # Leave some margin
            if split_point == -1: split_point = 4080 # Fallback if no newline found
            await message.reply_text(final_message[:split_point] + "\n\n...(list truncated due to length)", parse_mode=ParseMode.MARKDOWN)
        else:
            await message.reply_text(final_message, parse_mode=ParseMode.MARKDOWN)

    else: # Invalid action argument
        await message.reply_text(f"‚ö†Ô∏è Invalid action '{escape(action)}'.\n\n" + command_usage, parse_mode=ParseMode.MARKDOWN)

async def broadcast(update: Update, context: CallbackContext) -> None:
    """Sends a message to all known users (admin only). Supports Markdown/HTML."""
    user = update.effective_user
    message = update.message;

    # Admin & private chat checks
    if not user or not message or not ADMIN_ID or user.id != ADMIN_ID:
        logger.warning(f"Unauthorized access attempt to /broadcast by user {user.id if user else 'Unknown'}")
        if message: await message.reply_text("You are not authorized to use this command.")
        return
    if message.chat.type != 'private':
         await message.reply_text("Broadcast must be initiated from a private chat with the bot.")
         return

    message_to_send = ""
    parse_mode_to_use = None # Default to None (plain text)

    # Extract message content, allowing for replies to forward
    replied_message = message.reply_to_message
    if replied_message:
        # If replying, use the content of the replied message
        message_to_send = replied_message.text or replied_message.caption or ""
        # Prefer HTML if available, then MarkdownV2, then plain text
        if replied_message.text_html or replied_message.caption_html:
            parse_mode_to_use = ParseMode.HTML
            message_to_send = replied_message.text_html or replied_message.caption_html
            logger.debug("Using HTML parse mode for broadcast from replied message.")
        elif replied_message.text_markdown_v2 or replied_message.caption_markdown_v2:
             parse_mode_to_use = ParseMode.MARKDOWN_V2
             message_to_send = replied_message.text_markdown_v2 or replied_message.caption_markdown_v2
             logger.debug("Using MarkdownV2 parse mode for broadcast from replied message.")
        # TODO: Add handling for forwarding media if needed (more complex)
        # elif replied_message.photo or replied_message.video etc...
        #   await context.bot.copy_message(...) for each user?
    else:
        # If not replying, extract text after /broadcast command
        if message.text:
            text_content = message.text
            # Regex to strip command, case-insensitive, handles optional @botusername
            command_pattern = rf"^\s*/broadcast(?:@{context.bot.username})?\s+"
            message_to_send = re.sub(command_pattern, '', text_content, count=1, flags=re.IGNORECASE | re.DOTALL).strip()

            # Basic auto-detect formatting - prefer explicit formatting by admin
            # If entities exist, assume the admin used a specific format
            if message.entities:
                 if any(e.type in ['bold', 'italic', 'code', 'pre', 'text_link', 'strikethrough', 'underline', 'spoiler'] for e in message.entities):
                     # If standard entities are used, assume MarkdownV2 was intended
                     parse_mode_to_use = ParseMode.MARKDOWN_V2
                     message_to_send = message.text_markdown_v2 # Use the entity-parsed version
                     logger.debug(f"Detected MarkdownV2 entities, using MarkdownV2 for broadcast.")
                 # Could add HTML detection if needed, but MarkdownV2 is generally safer

    if not message_to_send:
         return await message.reply_text(
             "Usage: `/broadcast <Your message here>`\n"
             "Or reply to the message you want to broadcast with just `/broadcast`.\n"
             "(You should use MarkdownV2 or HTML formatting in your message).",
             parse_mode=ParseMode.MARKDOWN
         )

    known_users = load_known_users()
    if not known_users:
         return await message.reply_text("‚ÑπÔ∏è No known users found in the database to broadcast to.")

    total_users = len(known_users)

    logger.info(f"Admin {user.id} initiated broadcast to {total_users} users. ParseMode: {parse_mode_to_use}. Message (start): '{message_to_send[:50]}...'")
    status_message_obj = None
    try:
        status_message_obj = await message.reply_text(f"üì£ Broadcasting to {total_users} users... Starting now.")
    except TelegramError as e:
        logger.error(f"Failed to send initial broadcast status message: {e}")
        await message.reply_text(f"‚ö†Ô∏è Failed to start broadcast status tracking. Attempting to send anyway...")
        status_message_obj = None # Continue without status updates

    success, fail, blocked, other_fail = 0, 0, 0, 0
    start_time = time.time()
    # Adjust update frequency based on total users
    update_interval_count = max(10, min(100, total_users // 20 if total_users >= 200 else 10))
    last_status_update_time = time.time()

    user_list = list(known_users) # Convert set to list for indexed iteration
    users_to_remove = set() # Track users who block/deactivate

    for i, user_id_to_send in enumerate(user_list):
        # Skip sending to admin self if admin is in known_users
        if user_id_to_send == ADMIN_ID:
            logger.debug("Skipping broadcast to admin self.")
            continue

        sent = False
        try:
            await context.bot.send_message(
                chat_id=user_id_to_send,
                text=message_to_send,
                parse_mode=parse_mode_to_use,
                disable_web_page_preview=True # Generally good practice for broadcasts
            )
            success += 1
            sent = True
            logger.debug(f"Broadcast: Sent to {user_id_to_send}")
        except Forbidden: # Bot blocked by user or user deactivated
            blocked += 1
            users_to_remove.add(user_id_to_send)
            logger.debug(f"Broadcast: User {user_id_to_send} blocked the bot or is deactivated.")
        except BadRequest as e: # More specific errors
            error_str = str(e).lower()
            # Check for common reasons indicating user is unreachable permanently
            if any(sub in error_str for sub in ["chat not found", "user is deactivated", "bot was kicked", "user not found", "peer_id_invalid", "bot_blocked_by_user", "group chat was deactivated"]):
                 blocked += 1
                 users_to_remove.add(user_id_to_send)
                 logger.debug(f"Broadcast: User {user_id_to_send} unreachable ({error_str}). Marking for removal.")
            elif "can't parse entities" in error_str:
                 other_fail += 1
                 logger.warning(f"Broadcast parse error for {user_id_to_send}: {e}. Message may need fixing.")
                 # Consider stopping broadcast or trying plain text? For now, count as other fail.
            else: # Other BadRequest errors
                 other_fail += 1
                 logger.warning(f"Broadcast BadRequest for {user_id_to_send}: {e}")
        except TelegramError as e: # Catch other Telegram API errors
            # Could be temporary network issues, rate limits etc.
            logger.warning(f"Broadcast TelegramError for {user_id_to_send}: {e}")
            other_fail += 1
        except Exception as e: # Catch unexpected errors during send
            logger.error(f"Broadcast unexpected error for {user_id_to_send}: {e}", exc_info=True)
            other_fail += 1

        if not sent:
            fail += 1

        # --- Update Status Message Periodically ---
        current_time = time.time()
        processed_users = i + 1 # How many we've iterated through
        # Conditions to update: every N users OR every 10 seconds OR last user
        should_update_count = (update_interval_count > 0 and processed_users % update_interval_count == 0)
        should_update_time = (current_time - last_status_update_time > 10) # Check every 10 secs max
        is_last_user = (processed_users == total_users)

        if status_message_obj and (should_update_count or should_update_time or is_last_user):
             try:
                # Construct status text
                status_text = (f"üì£ Broadcasting... {processed_users}/{total_users}\n"
                               f"‚úÖ Sent: {success} | ‚ùå Failed: {fail} "
                               f"(üö´Blocked/Gone: {blocked}, ‚ùìOther: {other_fail})")
                await context.bot.edit_message_text(
                    chat_id=status_message_obj.chat_id, message_id=status_message_obj.message_id,
                    text=status_text
                )
                last_status_update_time = current_time # Reset timer
             except TelegramError as edit_err:
                  # Ignore "not modified" errors, log others
                  if "Message is not modified" not in str(edit_err):
                       logger.warning(f"Broadcast status update failed: {edit_err}")
                  last_status_update_time = current_time # Update time anyway to avoid spam

        # Small delay to avoid hitting rate limits, especially for large user bases
        await asyncio.sleep(0.05) # ~20 messages/second throttle

    # --- Final Broadcast Summary ---
    end_time = time.time()
    duration = format_time(end_time - start_time)
    removed_count = len(users_to_remove)
    save_status = "N/A"

    # Clean up known users list
    if users_to_remove:
        logger.info(f"Broadcast complete. Attempting to remove {removed_count} blocked/unreachable users from known list.")
        current_known_users = load_known_users() # Load fresh data
        cleaned_users = current_known_users - users_to_remove # Remove the bad ones
        if save_known_users(cleaned_users):
            final_user_count = len(cleaned_users)
            save_status = f"‚úÖ Removed {removed_count} inactive users ({final_user_count} remain)"
            logger.info(f"Saved cleaned known_users file after broadcast. {final_user_count} users remain.")
        else:
            save_status = f"‚ùå Save FAILED! ({removed_count} were marked for removal)"
            logger.error("Failed saving cleaned known_users file after broadcast.")
    else: # No users needed removal
         save_status = f"‚úÖ 0 users marked for removal ({total_users} remain)."

    logger.info(f"Broadcast finished. Sent: {success}, Failed: {fail} (Blocked:{blocked}, Other:{other_fail}). Cleanup: {save_status}. Duration: {duration}")

    # Construct final summary message for admin
    final_text = (
        f"üèÅ Broadcast Complete!\n\n"
        f"‚úÖ Messages Sent: {success}\n"
        f"‚ùå Send Failures: {fail}\n"
        f"   - üö´ Blocked/Gone: {blocked}\n"
        f"   - ‚ùì Other Errors: {other_fail}\n"
        f"üë• Total Users Attempted: {total_users}\n"
        f"üßπ User List Cleanup: {save_status}\n"
        f"‚è±Ô∏è Duration: {duration}"
    )

    # Send final summary (edit status message or send new)
    if status_message_obj:
        try:
            await context.bot.edit_message_text(
                chat_id=status_message_obj.chat_id, message_id=status_message_obj.message_id,
                text=final_text
            )
        except TelegramError as e:
            logger.warning(f"Failed to edit final broadcast status: {e}")
            await message.reply_text(final_text) # Send as new if edit fails
    else: # If status message failed initially
        await message.reply_text(final_text)

# --- Message Forwarding (Handle non-command, non-button messages) ---
async def forward_to_admin(update: Update, context: CallbackContext) -> None:
    """Forwards unhandled messages from non-admins in private chat to the admin."""
    user = update.effective_user
    message = update.message

    # Conditions for forwarding: Admin ID set, message exists, private chat, not from admin
    if not ADMIN_ID or ADMIN_ID == 0: return # Admin disabled
    if not user or not message: return # No user/message data
    if message.chat.type != 'private': return # Only forward from private chats
    if user.id == ADMIN_ID: return # Don't forward admin's own messages

    # Also check if the user is in the middle of a pending action we know about
    if context.user_data.get('pending_schedule'):
        # They might be sending text instead of the file
        await message.reply_text("I'm currently waiting for you to send the JSON *file* for your schedule. Please send the file or use /cancel.", reply_markup=main_reply_markup)
        return # Don't forward in this state
    if context.user_data.get('waiting_for_json'):
        await message.reply_text("I'm currently waiting for you to send the JSON *file* for manual processing. Please send the file or use /cancel.", reply_markup=main_reply_markup)
        return # Don't forward in this state


    # Add user to known users if they interact (even if message isn't handled)
    add_known_user(user.id)

    try:
        # Create a user info header for the forwarded message
        user_info = f"Forwarded message from: ID `{user.id}`"
        details = []
        if user.username: details.append(f"@{escape(user.username)}")
        if user.first_name: details.append(escape(user.first_name))
        if user.last_name: details.append(escape(user.last_name))
        if details: user_info += f" ({' '.join(details)})"

        # Send the header first
        await context.bot.send_message(ADMIN_ID, user_info, parse_mode=ParseMode.MARKDOWN)

        # Forward the actual message
        await context.bot.forward_message(
            chat_id=ADMIN_ID,
            from_chat_id=message.chat_id,
            message_id=message.message_id
        )
        logger.info(f"Forwarded message ID {message.message_id} from user {user.id} to admin {ADMIN_ID}")

    except Forbidden: # Bot blocked by admin
        logger.error(f"Failed to forward message to admin {ADMIN_ID}: Bot might be blocked by admin.")
        # Maybe disable forwarding temporarily? Or just log.
    except TelegramError as e:
         logger.error(f"Failed to forward message from user {user.id} to admin {ADMIN_ID} (TelegramError): {e}")
    except Exception as e:
         logger.error(f"Unexpected error forwarding message from user {user.id} to admin {ADMIN_ID}: {e}", exc_info=True)

# --- Global Error Handler ---
async def error_handler(update: object, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Log Errors caused by Updates and notify Admin."""
    logger.error(f"Exception while handling an update: {context.error}", exc_info=context.error)

    # --- Clean up potentially stuck user states on error ---
    user_notified = False
    if isinstance(update, Update) and update.effective_chat:
        chat_id_for_notify = update.effective_chat.id
        cleaned = False
        if context.user_data.pop('pending_schedule', None):
            logger.info(f"Cleared 'pending_schedule' state for chat {chat_id_for_notify} due to error.")
            cleaned = True
        if context.user_data.pop('waiting_for_json', None):
             logger.info(f"Cleared 'waiting_for_json' state for chat {chat_id_for_notify} due to error.")
             cleaned = True

        if cleaned:
             try:
                 await context.bot.send_message(
                     chat_id=chat_id_for_notify,
                     text="‚ö†Ô∏è An internal error occurred. Any pending action (like file scheduling or waiting for a file) has been cancelled. Please try again.",
                     reply_markup=main_reply_markup # Send keyboard with notification
                 )
                 user_notified = True
             except Exception as notify_err:
                  logger.error(f"Failed to notify user {chat_id_for_notify} about state cleanup after error: {notify_err}")

    # --- Notify Admin (if configured) ---
    if not ADMIN_ID or ADMIN_ID == 0:
        if not user_notified: # Only warn if user wasn't potentially notified
            logger.warning("Admin ID not set or invalid, cannot send error notification.")
        return

    # Format traceback
    tb_list = traceback.format_exception(None, context.error, context.error.__traceback__)
    tb_string = "".join(tb_list)

    try:
        # Format Update data (limit size, handle serialization issues)
        update_str = "Update data unavailable."
        if isinstance(update, Update):
             try:
                 # Simplify update dict for logging (remove sensitive/large fields if needed)
                 update_data = update.to_dict()
                 # Example simplification: just show message type and user ID
                 if 'message' in update_data and update_data['message']:
                     msg = update_data['message']
                     user_id_from_update = msg.get('from_user', {}).get('id', 'unknown')
                     update_data_simple = {'update_id': update_data.get('update_id'), 'message': {'message_id': msg.get('message_id'), 'chat_id': msg.get('chat',{}).get('id'), 'from_user_id': user_id_from_update, 'type': msg.get('document',{}).get('mime_type') or ('text' if 'text' in msg else 'other') }}
                     update_str = json.dumps(update_data_simple, indent=1, ensure_ascii=False, default=str)
                 else: # Handle other update types like callback queries
                     update_str = json.dumps(update_data, indent=1, ensure_ascii=False, default=str, skipkeys=True)

             except Exception as json_err:
                 logger.error(f"Could not serialize update object to JSON: {json_err}")
                 update_str = str(update) # Fallback to string representation
        elif update: update_str = str(update)

        # Format Context data (limit size)
        context_str = "Context data unavailable or complex."
        try:
            # Limit size of potentially large user/chat data strings
            # Use pformat for potentially better formatting of context data
            from pprint import pformat
            chat_data_str = pformat(context.chat_data, width=80, depth=2)[:300] + ('...' if len(pformat(context.chat_data)) > 300 else '')
            user_data_str = pformat(context.user_data, width=80, depth=2)[:300] + ('...' if len(pformat(context.user_data)) > 300 else '')
            bot_data_str = pformat(context.bot_data, width=80, depth=2)[:200] + ('...' if len(pformat(context.bot_data)) > 200 else '')

            context_info = {
                "args": str(context.args) if hasattr(context, 'args') else 'N/A',
                "user_data": user_data_str,
                "chat_data": chat_data_str,
                "bot_data": bot_data_str,
            }
            context_str = json.dumps(context_info, indent=1, default=str)
        except Exception as ctx_err:
            context_str = f"Error getting context data: {ctx_err}"

        # Limit lengths for Telegram message
        max_len_tb = 3000
        max_len_update = 500 # Reduced update size for clarity
        max_len_context = 300 # Reduced context size

        # Escape HTML characters in user-generated/error content
        error_escaped = escape(str(context.error))
        error_type_escaped = escape(type(context.error).__name__)
        update_short = escape(update_str[:max_len_update] + ('...' if len(update_str) > max_len_update else ''))
        context_short = escape(context_str[:max_len_context] + ('...' if len(context_str) > max_len_context else ''))
        # Take the end of the traceback which is usually most relevant
        tb_short = escape(tb_string[-max_len_tb:])

        # Construct HTML formatted message for admin
        error_message = (
            f"‚ö†Ô∏è <b>Bot Error Encountered</b> ‚ö†Ô∏è\n\n"
            f"<b>Error:</b>\n<pre>{error_escaped}</pre>\n"
            f"<b>Type:</b> <pre>{error_type_escaped}</pre>\n\n"
            f"<b>Update (limited):</b>\n<pre>{update_short}</pre>\n\n"
            f"<b>Context (limited):</b>\n<pre>{context_short}</pre>\n\n"
            f"<b>Traceback (end, limited):</b>\n<pre>{tb_short}</pre>"
        )

        max_msg_len = 4096 # Telegram message length limit
        if len(error_message) > max_msg_len:
             logger.warning(f"Error message length ({len(error_message)}) exceeds limit. Sending in parts.")
             # Send in chunks if too long
             for i in range(0, len(error_message), max_msg_len):
                 chunk = error_message[i:i + max_msg_len]
                 await context.bot.send_message(ADMIN_ID, chunk, parse_mode=ParseMode.HTML)
                 await asyncio.sleep(0.5) # Small delay between chunks
        else:
            await context.bot.send_message(ADMIN_ID, error_message, parse_mode=ParseMode.HTML)

    except Forbidden: # Bot blocked by admin
        logger.critical(f"CRITICAL: Cannot send error notification to admin {ADMIN_ID}. Bot might be blocked by the admin.")
    except Exception as e: # Error sending the error message itself
        logger.critical(f"CRITICAL: Failed to send detailed error notification to admin: {e}", exc_info=True)
        # Send a very basic fallback message
        try:
            fallback_msg = f"Bot encountered a critical error. Check logs immediately!\nError type: {type(context.error).__name__}\nError: {context.error}"
            await context.bot.send_message(ADMIN_ID, fallback_msg[:4090]) # Truncate fallback too
        except Exception as fallback_err:
            # If even the fallback fails, log it and give up on notifying
            logger.critical(f"CRITICAL: Failed even the simplest error notification to admin. Check logs manually. Fallback error: {fallback_err}")

# --- Background Task for Scheduled Processing (NEW) ---

async def run_scheduled_file_processor(application: Application) -> None:
    """Periodically checks for scheduled files and processes them."""
    bot = application.bot
    logger.info(f"Background scheduler started. Check interval: {AUTO_PROCESS_CHECK_INTERVAL}s")
    # Initial short delay to allow bot to fully start up and maybe load initial data
    await asyncio.sleep(15)

    while True:
        try: # Wrap the entire loop in a try-except to prevent scheduler death
            now_utc = datetime.now(timezone.utc)
            logger.debug(f"Scheduler check running at {now_utc.isoformat()}")
            schedules = load_scheduled_files()
            if not schedules:
                logger.debug("Scheduler: No schedules found.")
                await asyncio.sleep(AUTO_PROCESS_CHECK_INTERVAL)
                continue

            tasks_to_run = [] # Collect processing tasks for this cycle

            # Load GitHub configs once per cycle
            github_configs = load_github_configs()

            # Iterate through a copy of user IDs to allow modification during iteration if needed (e.g., remove user)
            for user_id_str in list(schedules.keys()):
                user_schedules = schedules.get(user_id_str)
                if not isinstance(user_schedules, dict):
                    logger.warning(f"Scheduler: Invalid schedule data format for user {user_id_str}. Skipping.")
                    continue # Skip invalid user entries

                try:
                    user_id = int(user_id_str)
                except ValueError:
                    logger.warning(f"Scheduler: Invalid user ID key '{user_id_str}' in schedules. Skipping.")
                    continue

                if not is_user_vip(user_id): # Double check VIP status each cycle
                     logger.info(f"Scheduler: User {user_id} is no longer VIP. Skipping their schedules.")
                     # Optionally: could auto-remove schedule here, but might be better to leave for manual cleanup or expiry check
                     # E.g., remove_all_schedules_for_user(user_id)
                     continue

                user_github_config = github_configs.get(user_id_str) # Get user's specific GitHub config

                # Iterate through a copy of schedule names for this user
                for schedule_name in list(user_schedules.keys()):
                    schedule_info = user_schedules.get(schedule_name)
                    if not isinstance(schedule_info, dict):
                        logger.warning(f"Scheduler: Invalid schedule entry '{schedule_name}' for user {user_id}. Skipping.")
                        continue # Skip invalid schedule entries

                    next_run_iso = schedule_info.get('next_run_time_iso')
                    stored_file_path = schedule_info.get('stored_file_path')
                    interval_seconds = schedule_info.get('interval_seconds')

                    # Basic validation of essential fields
                    if not next_run_iso or not stored_file_path or not interval_seconds:
                        logger.warning(f"Scheduler: Skipping invalid schedule '{schedule_name}' for user {user_id} (missing essential info).")
                        continue

                    try:
                        next_run_dt = datetime.fromisoformat(next_run_iso.replace('Z', '+00:00'))
                    except (ValueError, TypeError):
                        logger.warning(f"Scheduler: Skipping schedule '{schedule_name}' for user {user_id} due to invalid next_run_time_iso: {next_run_iso}")
                        continue

                    # --- Check if schedule is due ---
                    if next_run_dt <= now_utc:
                        logger.info(f"Scheduler: Schedule '{schedule_name}' for user {user_id} is due. Preparing task.")
                        # Ensure stored file actually exists before queuing task
                        if os.path.exists(stored_file_path):
                            tasks_to_run.append(
                                process_single_schedule(
                                    bot, user_id, schedule_name, schedule_info, user_github_config
                                )
                            )
                        else:
                            logger.error(f"Scheduler: Stored file missing for due schedule '{schedule_name}' user {user_id} at path {stored_file_path}. Skipping run and notifying user.")
                            try:
                                await bot.send_message(
                                    user_id,
                                    f"‚ö†Ô∏è Error: Could not run scheduled task `'{escape(schedule_info.get('user_schedule_name', schedule_name))}'`. The associated data file seems to be missing. Please use `/setfile` again for this schedule.",
                                    parse_mode=ParseMode.MARKDOWN
                                )
                                # Should we remove the broken schedule automatically? Risky. Let user fix.
                                # Maybe just update next_run far in future?
                            except Exception as notify_err:
                                logger.error(f"Scheduler: Failed to notify user {user_id} about missing schedule file: {notify_err}")
                            # Skip processing this one, but don't update its time yet

            # --- Run due tasks concurrently ---
            if tasks_to_run:
                logger.info(f"Scheduler: Running {len(tasks_to_run)} due schedule(s).")
                # Gather results, return_exceptions=True prevents one task failure from stopping others
                results = await asyncio.gather(*tasks_to_run, return_exceptions=True)

                # --- Update schedule times after processing ---
                current_schedules = load_scheduled_files() # Reload fresh data before update
                made_changes = False
                for result in results:
                     if isinstance(result, tuple) and len(result) == 3:
                        res_user_id_str, res_schedule_name, _ = result # Ignore success status for now
                        try:
                            # Check if schedule still exists (might have been removed manually during run)
                            if res_user_id_str in current_schedules and res_schedule_name in current_schedules[res_user_id_str]:
                                info = current_schedules[res_user_id_str][res_schedule_name]
                                interval_s = info.get('interval_seconds')
                                if interval_s and isinstance(interval_s, int) and interval_s > 0:
                                    last_run_time = datetime.now(timezone.utc) # Record actual completion time
                                    next_run_time = last_run_time + timedelta(seconds=interval_s)
                                    current_schedules[res_user_id_str][res_schedule_name]['last_run_time_iso'] = last_run_time.isoformat()
                                    current_schedules[res_user_id_str][res_schedule_name]['next_run_time_iso'] = next_run_time.isoformat()
                                    made_changes = True
                                    logger.info(f"Scheduler: Updated next run time for '{res_schedule_name}' (User {res_user_id_str}) to {next_run_time.isoformat()}")
                                else:
                                     logger.error(f"Scheduler: Cannot update next run for '{res_schedule_name}' (User {res_user_id_str}) - missing or invalid interval.")
                            else:
                                 logger.info(f"Scheduler: Schedule '{res_schedule_name}' for user {res_user_id_str} was removed before run time update.")
                        except Exception as update_err:
                            logger.error(f"Scheduler: Error updating schedule info for User {res_user_id_str}, Schedule {res_schedule_name}: {update_err}", exc_info=True)

                     elif isinstance(result, Exception):
                         # Log the exception that occurred within a task
                         # The task itself should have logged details and notified the user
                         logger.error(f"Scheduler: Error result returned from scheduled task processing: {result}", exc_info=result)
                         # Don't update run time for failed tasks to allow potential retry or manual check

                if made_changes:
                    if not save_scheduled_files(current_schedules):
                        logger.error("Scheduler: CRITICAL - Failed to save updated schedule run times!")

            else: # No tasks were run
                logger.debug("Scheduler: No schedules were due this cycle.")

        except Exception as loop_err:
            logger.critical(f"Scheduler: Unhandled exception in main processing loop: {loop_err}", exc_info=True)
            # Avoid busy-looping if there's a persistent error
            await asyncio.sleep(60)

        # Wait for the next check interval regardless of outcome (unless error caused sleep above)
        await asyncio.sleep(AUTO_PROCESS_CHECK_INTERVAL)

async def process_single_schedule(bot, user_id: int, schedule_name: str, schedule_info: dict, github_config: dict | None) -> tuple[str, str, bool]:
    """
    Processes a single scheduled file: reads data, calls API, uploads to GitHub.
    Returns (user_id_str, schedule_name, github_upload_success_or_skipped)
    """
    user_id_str = str(user_id)
    stored_file_path = schedule_info.get('stored_file_path')
    user_display_name = schedule_info.get('user_schedule_name', schedule_name) # For messages
    github_upload_status = False # Default to false (meaning upload failed or was not needed)

    log_prefix = f"AutoProcess User {user_id} Schedule '{schedule_name}':"
    logger.info(f"{log_prefix} Starting.")

    notify_parts = [f"‚öôÔ∏è Auto-processing started for schedule `'{escape(user_display_name)}'`..."]
    status_msg_obj = None
    try:
        # Send initial status (no reply markup)
        status_msg_obj = await bot.send_message(user_id, notify_parts[0], parse_mode=ParseMode.MARKDOWN)
    except Exception as e:
        logger.error(f"{log_prefix} Failed to send initial status DM: {e}")
        # Proceed without status updates if initial message fails

    accounts_data = []
    # Create a unique temporary directory for this specific run's results
    run_timestamp = int(time.time())
    temp_results_dir = os.path.join(TEMP_DIR, f"auto_{user_id}_{schedule_name}_{run_timestamp}")
    cleanup_paths_auto = [temp_results_dir] # Add main temp dir for cleanup
    jwt_token_path_for_upload = None

    try:
        # --- 1. Read and Validate Stored File ---
        if not stored_file_path or not os.path.exists(stored_file_path):
            raise FileNotFoundError(f"Stored file path missing or file not found: {stored_file_path}")

        await update_schedule_status(bot, status_msg_obj, notify_parts, "Reading stored file...")

        with open(stored_file_path, 'r', encoding='utf-8') as f:
            try:
                accounts_data = json.load(f)
                if not isinstance(accounts_data, list):
                    raise ValueError("Scheduled file content must be a JSON list `[...]`.")
                if accounts_data and not all(isinstance(item, dict) for item in accounts_data):
                     # Be more specific about the error
                     first_bad = next((x for x in accounts_data if not isinstance(x, dict)), None)
                     raise ValueError(f"All items in the list must be JSON objects (`{{...}}`). Found: {type(first_bad)}")
                logger.info(f"{log_prefix} Read {len(accounts_data)} accounts from {stored_file_path}")
            except json.JSONDecodeError as e:
                raise ValueError(f"Invalid JSON format in stored file: {e.msg} (Line: {e.lineno}, Col: {e.colno})")
            except ValueError as ve: # Catch our structure validation error
                 raise ve
            except Exception as read_err:
                 raise IOError(f"Could not read stored file: {read_err}")


        total_count = len(accounts_data)
        if total_count == 0:
            logger.info(f"{log_prefix} Stored file is empty. No processing needed.")
            await update_schedule_status(bot, status_msg_obj, notify_parts, "‚úÖ Finished: Stored file was empty.")
            # Return success because the task ran, even if there was nothing to do
            return user_id_str, schedule_name, True # github_upload_status is implicitly True (skipped)

        # --- 2. Process Accounts via API ---
        await update_schedule_status(bot, status_msg_obj, notify_parts, f"Processing {total_count} accounts via API...")

        start_time = time.time()
        successful_tokens = []
        working_accounts = [] # Keep track for potential future use (e.g., detailed logs)
        lost_accounts = []
        errors_summary = defaultdict(int)
        processed_count = 0

        semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
        async with aiohttp.ClientSession() as session:
            tasks = [process_account(session, account, semaphore) for account in accounts_data]

            for i, future in enumerate(asyncio.as_completed(tasks)):
                # Progress update within processing loop (less frequent than manual)
                update_freq_auto = max(5, min(50, total_count // 10)) # Update more granularly for auto tasks maybe
                if status_msg_obj and (i + 1) % update_freq_auto == 0:
                     progress_pct = ((i + 1) / total_count) * 100
                     await update_schedule_status(bot, status_msg_obj, notify_parts, f"Processing API... {i+1}/{total_count} ({progress_pct:.0f}%)", keep_last=False)

                try:
                    token, region, working_acc, lost_acc, error_reason = await future
                    processed_count += 1
                    if token and working_acc:
                        successful_tokens.append({"token": token, "region": region})
                        working_accounts.append(working_acc)
                    elif lost_acc:
                        lost_accounts.append(lost_acc)
                        reason = lost_acc.get("error_reason", "Unknown")
                        errors_summary[reason.split(':')[0].strip()] += 1
                    else: # Should not happen
                         lost_accounts.append({"account_info": "unknown", "error_reason": "Unexpected process_account result"})
                         errors_summary["Processing function error"] += 1
                except Exception as task_err:
                    processed_count += 1
                    logger.error(f"{log_prefix} Error retrieving result from API task: {task_err}", exc_info=True)
                    lost_accounts.append({"account_info": "unknown", "error_reason": f"Task Error: {task_err}"})
                    errors_summary["Internal task error"] += 1
                    # Try to continue processing other accounts

        processing_time = time.time() - start_time
        logger.info(f"{log_prefix} API processing finished in {processing_time:.2f}s. Success: {len(successful_tokens)}, Failed: {len(lost_accounts)}")
        notify_parts.append(f"üìä API Results: {len(successful_tokens)} tokens generated, {len(lost_accounts)} failures.")
        if errors_summary:
             # Show top 1-2 errors in notification briefly
             top_errors = sorted(errors_summary.items(), key=lambda item: item[1], reverse=True)
             # ******** CORRECTED VARIABLE NAME ********
             error_snippets = []
             for err_msg, count in top_errors[:2]:
                 error_snippets.append(f"`{escape(err_msg)}` ({count})")
             notify_parts.append(f"   (Top errors: {'; '.join(error_snippets)})")


        # --- 3. Prepare Token File for Upload ---
        if successful_tokens:
            await update_schedule_status(bot, status_msg_obj, notify_parts, "Preparing token file for upload...")
            os.makedirs(temp_results_dir, exist_ok=True) # Ensure temp dir for this run exists
            # Use a unique name within the run's temp dir
            jwt_token_path_for_upload = os.path.join(temp_results_dir, 'jwt_token_auto.json')
            # Filter out potential null/empty tokens just in case
            tokens_only_list = [{"token": entry["token"]} for entry in successful_tokens if entry.get("token")]
            if tokens_only_list:
                if not save_json_data(jwt_token_path_for_upload, tokens_only_list):
                    jwt_token_path_for_upload = None # Nullify if save failed
                    raise IOError("Failed to save temporary token file for upload.")
                logger.info(f"{log_prefix} Saved {len(tokens_only_list)} tokens to {jwt_token_path_for_upload}")
            else:
                jwt_token_path_for_upload = None # No valid tokens to save
                logger.info(f"{log_prefix} No valid tokens found to save for upload, although processing reported successes.")
                notify_parts.append("‚ÑπÔ∏è No valid tokens generated, skipping GitHub upload.")
                github_upload_status = True # Skipped is considered 'success' in terms of the schedule run
        else:
            logger.info(f"{log_prefix} No successful tokens generated. Skipping GitHub upload prep.")
            notify_parts.append("‚ÑπÔ∏è No successful tokens, skipping GitHub upload.")
            github_upload_status = True # Skipped is 'success'

        # --- 4. Trigger GitHub Upload ---
        # Check if token file was created AND github config exists and is valid
        if jwt_token_path_for_upload and github_config and isinstance(github_config, dict):
            await update_schedule_status(bot, status_msg_obj, notify_parts, "Attempting GitHub upload...")
            # Call the background-safe upload function
            upload_success = await upload_to_github_background(
                bot, user_id, jwt_token_path_for_upload, github_config
            )
            github_upload_status = upload_success
            # upload_to_github_background sends its own detailed status messages
            # We just record the final success/failure here
            logger.info(f"{log_prefix} GitHub upload finished. Success: {upload_success}")
            # The upload function updates the status_msg_obj, so no need to add more here usually.
        elif jwt_token_path_for_upload:
            # Tokens generated but no valid GitHub config
            logger.info(f"{log_prefix} Tokens generated but GitHub not configured or config invalid. Skipping upload.")
            # Add notification about skipping due to config issue
            notify_parts.append("‚ÑπÔ∏è GitHub upload skipped (not configured or config invalid). Use /mygithub & /setgithub.")
            await update_schedule_status(bot, status_msg_obj, notify_parts, "Skipped GitHub upload (no config).")
            github_upload_status = True # Skipped due to config is 'success' for scheduler
        # else: Upload was already skipped because no tokens were generated


        # --- 5. Final Notification ---
        final_status_line = "‚úÖ Auto-processing completed."
        # Only explicitly mention failure if upload was *attempted* but failed
        if jwt_token_path_for_upload and github_config and not github_upload_status:
             final_status_line = "‚ö†Ô∏è Auto-processing completed, but GitHub upload failed (see details above)."

        await update_schedule_status(bot, status_msg_obj, notify_parts, final_status_line, is_final=True)
        logger.info(f"{log_prefix} Finished. Overall Success (for scheduler): {github_upload_status or (not jwt_token_path_for_upload)}")

        # Return True if upload succeeded OR if it was correctly skipped (no tokens or no config)
        # Return False only if upload was attempted and failed.
        final_success_state = github_upload_status or (not jwt_token_path_for_upload)

    except Exception as e:
        logger.error(f"{log_prefix} FAILED: {e}", exc_info=True)
        final_success_state = False # Mark as failed on any exception during processing
        try:
            # Send error notification to user
            error_msg = f"‚ùå **FAILED:** Auto-processing for schedule `'{escape(user_display_name)}'` encountered an error:\n`{escape(str(e))}`"
            # Try to edit the status message, otherwise send new
            if status_msg_obj:
                 await update_schedule_status(bot, status_msg_obj, notify_parts, error_msg, is_final=True)
            else:
                await bot.send_message(user_id, error_msg, parse_mode=ParseMode.MARKDOWN)
        except Exception as notify_err:
            logger.error(f"{log_prefix} Could not notify user about processing failure: {notify_err}")

    finally:
        # Clean up temporary files/directory for this run
        for path in cleanup_paths_auto:
            if os.path.exists(path):
                try:
                    if os.path.isdir(path):
                        shutil.rmtree(path)
                    else:
                        os.remove(path)
                    logger.debug(f"{log_prefix} Cleaned up temp path: {path}")
                except OSError as e:
                    logger.warning(f"{log_prefix} Could not clean up temp path {path}: {e}")

    # Return schedule identifiers and final success status
    return user_id_str, schedule_name, final_success_state

async def update_schedule_status(bot, status_msg_obj, notify_parts: list, new_status: str, keep_last=True, is_final=False):
    """Helper to update the status message sent to the user during auto-processing."""
    if not status_msg_obj: return # Cannot update if initial message failed

    if keep_last and len(notify_parts) > 1:
        # Replace the last status line if keep_last is True
        notify_parts[-1] = new_status
    else:
        # Append new status line
        notify_parts.append(new_status)

    # Limit message history for brevity (e.g., keep first line + last 5 status lines)
    max_lines = 7
    if len(notify_parts) > max_lines:
        # Keep first line (header) and last N lines
        notify_parts = [notify_parts[0]] + notify_parts[-(max_lines-1):]
        # Add ellipsis if lines were removed between header and tail
        if notify_parts[1] != "...": notify_parts.insert(1, "...")

    message_text = "\n".join(notify_parts)
    # ******** CORRECTED: REMOVED reply_markup from edit *********
    # Reply keyboard should not be attached when editing.
    # final_markup = main_reply_markup if is_final else None # <-- REMOVED

    try:
        await bot.edit_message_text(
            chat_id=status_msg_obj.chat_id,
            message_id=status_msg_obj.message_id,
            text=message_text[:4096], # Ensure text doesn't exceed limit
            parse_mode=ParseMode.MARKDOWN,
            # reply_markup=final_markup # <-- REMOVED
        )
    except TelegramError as e:
        # Ignore "not modified" error, log others
        if "Message is not modified" not in str(e) and "message to edit not found" not in str(e).lower():
            logger.warning(f"AutoProcess: Failed to edit status message {status_msg_obj.message_id}: {e}")
        # If message not found, maybe user deleted it. Stop trying to edit.
        if "message to edit not found" in str(e).lower():
            logger.warning(f"AutoProcess: Status message {status_msg_obj.message_id} was deleted by user? Stopping updates.")
            status_msg_obj = None # Prevent further edits


# --- Main Application Setup ---

async def main() -> None:
    """Initialize data, set up handlers, start scheduler, and run the bot."""
    global ADMIN_ID, TOKEN

    print("\n--- Initializing Bot ---")

    # --- Essential Config Checks ---
    if not TOKEN or TOKEN == "YOUR_FALLBACK_BOT_TOKEN":
        print("\n" + "="*60)
        print(" FATAL ERROR: TELEGRAM_BOT_TOKEN is missing or invalid.")
        print(" Please set the TELEGRAM_BOT_TOKEN environment variable or")
        print(" update the TOKEN variable directly in the script.")
        print(" -> Exiting.")
        print("="*60 + "\n")
        exit(1)
    elif len(TOKEN.split(':')) != 2:
        print("\n" + "="*60)
        print(f" FATAL ERROR: TELEGRAM_BOT_TOKEN format looks incorrect ('{TOKEN[:10]}...'). Should be 'ID:SECRET'.")
        print(" -> Exiting.")
        print("="*60 + "\n")
        exit(1)


    try:
        admin_id_env = os.getenv('ADMIN_ID')
        if admin_id_env and admin_id_env.isdigit():
            ADMIN_ID = int(admin_id_env)
            if ADMIN_ID == 0: print(" WARNING: ADMIN_ID is set to 0. Admin features disabled.")
            else: logger.info(f"Admin User ID configured: {ADMIN_ID}")
        else:
            if isinstance(ADMIN_ID, int) and ADMIN_ID != 0: print(f" WARNING: ADMIN_ID not set or invalid in environment. Using script default: {ADMIN_ID}")
            else:
                ADMIN_ID = 0
                print(" WARNING: ADMIN_ID not set/invalid/0. Setting to 0. Admin features disabled.")
    except Exception as e:
         ADMIN_ID = 0
         print(f" WARNING: Error processing ADMIN_ID from environment ({e}). Setting to 0. Admin features disabled.")

    if not API_BASE_URL: logger.warning("JWT_API_URL not set, using default.")
    else: logger.info(f"Using API Base URL: {API_BASE_URL}")
    if not API_KEY or API_KEY == 'atxdev': logger.warning("JWT_API_KEY not set or using default.")

    # --- Create Directories ---
    try:
        os.makedirs(DATA_DIR, exist_ok=True)
        os.makedirs(TEMP_DIR, exist_ok=True)
        os.makedirs(SCHEDULED_FILES_DATA_DIR, exist_ok=True) # Create schedule data dir
        logger.info(f"Data Directory: {DATA_DIR}")
        logger.info(f"Temp Directory: {TEMP_DIR}")
        logger.info(f"Scheduled Files Storage: {SCHEDULED_FILES_DATA_DIR}")
    except OSError as e:
        print(f"\nFATAL ERROR: Cannot create required directories: {e}\n-> Exiting.")
        exit(1)

    # --- Build Application ---
    # Configure connection pool size and timeouts
    app_builder = Application.builder().token(TOKEN)\
        .concurrent_updates(True) \
        .read_timeout(30) \
        .write_timeout(30) \
        .connect_timeout(30) \
        .pool_timeout(60) \
        .get_updates_read_timeout(40) \
        .get_updates_pool_timeout(70)

    application = app_builder.build()

    # --- Handlers ---
    private_chat_filter = filters.ChatType.PRIVATE

    # Core Commands & Buttons
    application.add_handler(CommandHandler("start", start, filters=private_chat_filter))
    application.add_handler(CommandHandler("help", help_command, filters=private_chat_filter))
    application.add_handler(MessageHandler(filters.Regex(f"^{re.escape(COMMAND_BUTTONS_LAYOUT[2][1])}$") & private_chat_filter, help_command)) # Help Button

    # VIP Info Commands & Buttons
    application.add_handler(CommandHandler("vipstatus", vip_status_command, filters=private_chat_filter))
    application.add_handler(MessageHandler(filters.Regex(f"^{re.escape(COMMAND_BUTTONS_LAYOUT[0][1])}$") & private_chat_filter, vip_status_command)) # Vip Status Button
    application.add_handler(CommandHandler("vipshop", vip_shop_command, filters=private_chat_filter))
    application.add_handler(MessageHandler(filters.Regex(f"^{re.escape(COMMAND_BUTTONS_LAYOUT[1][0])}$") & private_chat_filter, vip_shop_command)) # Vip Shop Button

    # GitHub Config Commands & Button (VIP)
    application.add_handler(CommandHandler("setgithub", set_github_direct, filters=private_chat_filter))
    application.add_handler(CommandHandler("mygithub", my_github_config, filters=private_chat_filter))
    application.add_handler(MessageHandler(filters.Regex(f"^{re.escape(COMMAND_BUTTONS_LAYOUT[1][1])}$") & private_chat_filter, my_github_config)) # GitHub Status Button

    # Scheduled File Commands & Button (VIP)
    application.add_handler(CommandHandler("setfile", set_scheduled_file_start, filters=private_chat_filter))
    application.add_handler(CommandHandler("removefile", remove_scheduled_file, filters=private_chat_filter))
    application.add_handler(CommandHandler("scheduledfiles", list_scheduled_files, filters=private_chat_filter))
    application.add_handler(MessageHandler(filters.Regex(f"^{re.escape(COMMAND_BUTTONS_LAYOUT[2][0])}$") & private_chat_filter, list_scheduled_files)) # Scheduled Files Button

    # Manual File Processing (handles button click AND direct file send)
    # IMPORTANT: This handler now ALSO catches files sent for scheduling.
    # The `handle_document` function itself routes based on `context.user_data['pending_schedule']`.
    application.add_handler(MessageHandler(filters.Text(COMMAND_BUTTONS_LAYOUT[0][0]) & private_chat_filter, handle_document)) # Process File Button
    application.add_handler(MessageHandler(
        # Match application/json OR filename ending .json (case insensitive)
        (filters.Document.MimeType('application/json') | filters.Document.FileExtension('json')) & private_chat_filter,
        handle_document # Catches both manual sends and sends after /setfile
    ))

    # Cancel Command & Button
    application.add_handler(CommandHandler("cancel", cancel, filters=private_chat_filter))
    application.add_handler(MessageHandler(filters.Regex(f"^{re.escape(COMMAND_BUTTONS_LAYOUT[3][0])}$") & private_chat_filter, cancel)) # Cancel Button


    # Admin Commands (only add if ADMIN_ID is valid)
    if ADMIN_ID and ADMIN_ID != 0:
        admin_filter = filters.User(user_id=ADMIN_ID) & private_chat_filter
        application.add_handler(CommandHandler("vip", vip_management, filters=admin_filter))
        application.add_handler(CommandHandler("broadcast", broadcast, filters=admin_filter))
        logger.info(f"Admin commands (/vip, /broadcast) enabled for ADMIN_ID: {ADMIN_ID}.")
    else:
         logger.warning("Admin commands are disabled as ADMIN_ID is not set, 0, or invalid.")

    # Message Forwarding to Admin (if configured)
    if ADMIN_ID and ADMIN_ID != 0:
        # Define filters for messages that should be forwarded
        known_button_texts_set = {btn for row in COMMAND_BUTTONS_LAYOUT for btn in row}
        # Exclude commands, known buttons, JSON documents (handled by handle_document), and messages from admin
        # Also explicitly exclude messages handled by other handlers if needed
        forwarding_filters = (
            private_chat_filter &
            ~filters.User(user_id=ADMIN_ID) &
            ~filters.COMMAND &
            ~filters.Text(known_button_texts_set) &
            ~(filters.Document.MimeType('application/json') | filters.Document.FileExtension('json')) &
            (filters.TEXT | filters.PHOTO | filters.Sticker.ALL | filters.VIDEO | filters.VIDEO_NOTE | filters.VOICE | filters.AUDIO | filters.Document.ALL) & # Forward various message types
            ~filters.UpdateType.EDITED_MESSAGE # Don't forward edits
        )
        application.add_handler(MessageHandler(forwarding_filters, forward_to_admin))
        logger.info("Message forwarding to admin enabled.")
    else:
        logger.warning("Message forwarding to admin is disabled as ADMIN_ID is not set or invalid.")

    # Error Handler (must be last handler)
    application.add_error_handler(error_handler)

    logger.info("ü§ñ Bot is initializing and connecting to Telegram...")
    print("\n" + "="*60)
    print(" üöÄ Advanced JWT Token Bot with File Scheduling is starting...")

    try:
        # Initialize application (connects, gets bot info etc.)
        await application.initialize()

        bot_info = await application.bot.get_me()
        print(f" ‚úîÔ∏è Bot Username: @{bot_info.username} (ID: {bot_info.id})")
        print(f" ‚úîÔ∏è Admin ID: {ADMIN_ID if (ADMIN_ID and ADMIN_ID != 0) else 'Not Set (Admin Features Disabled)'}")
        print(f" ‚úîÔ∏è Data Directory: {DATA_DIR}")
        print(f" ‚úîÔ∏è Scheduled File Check Interval: {AUTO_PROCESS_CHECK_INTERVAL}s")

        # Start the background scheduler task only AFTER application is initialized
        # Pass the application object so the task can access application.bot etc.
        scheduler_task = asyncio.create_task(run_scheduled_file_processor(application))
        logger.info("Background scheduler task created.")

        # Start polling for updates from Telegram
        await application.start()
        await application.updater.start_polling(allowed_updates=Update.ALL_TYPES, drop_pending_updates=True)
        print("\n Bot is now polling for updates. Press Ctrl+C to stop.")
        print("="*60 + "\n")

        # Keep the main thread alive (polling runs in background)
        # Wait for the scheduler task to finish (it won't normally, unless cancelled)
        await scheduler_task

    except (TelegramError, ConnectionError) as e:
         print("\n" + "="*60)
         print(f" FATAL ERROR: Could not connect to Telegram or initialize bot.")
         print(f" Error: {e}")
         print(" Please check your network connection and bot token.")
         print(" -> Exiting.")
         print("="*60 + "\n")
         logger.critical(f"Failed to initialize or start polling: {e}", exc_info=True)
         exit(1)
    except asyncio.CancelledError:
        logger.info("Main task or scheduler was cancelled.")
        # Shutdown should happen in finally block
    except Exception as e:
        print("\n" + "="*60)
        print(f" FATAL ERROR: An unexpected error occurred during bot startup or main loop.")
        print(f" Error: {e}")
        print(" -> Exiting.")
        print("="*60 + "\n")
        logger.critical(f"Unhandled exception during startup/runtime: {e}", exc_info=True)
        exit(1)
    finally:
         # Attempt graceful shutdown on any exit path (normal or error)
         if 'application' in locals() and application.running:
              logger.info("Attempting graceful shutdown...")
              await application.stop()
              await application.shutdown()
              logger.info("Application stopped.")
         if 'scheduler_task' in locals() and not scheduler_task.done():
              logger.info("Cancelling scheduler task...")
              scheduler_task.cancel()
              try:
                   await scheduler_task # Allow cancellation to propagate
              except asyncio.CancelledError:
                   logger.info("Scheduler task cancelled successfully.")
              except Exception as task_err:
                  logger.error(f"Error during scheduler task cancellation/await: {task_err}")
         logger.info("Shutdown complete.")


if __name__ == '__main__':
    try:
        # Ensure TOKEN has a valid value before running
        if not TOKEN or TOKEN == "YOUR_FALLBACK_BOT_TOKEN":
             print("FATAL: TELEGRAM_BOT_TOKEN is not set. Please configure it before running.")
        else:
             asyncio.run(main())
    except KeyboardInterrupt:
        print("\n-- Bot stopping due to Ctrl+C --")
        logger.info("Bot stopped manually via KeyboardInterrupt.")
    except Exception as e:
        print(f"\nüí• A critical unhandled exception occurred outside the main asyncio loop: {e}")
        logger.critical(f"Critical unhandled exception in __main__: {e}", exc_info=True)



# ===================== FINAL MULTI-GITHUB + ADMIN EXTENSION =====================
# This section ONLY ADDS features. Original logic remains untouched.

# -------- GitHub config load/save --------
def load_github_configs():
    # { user_id: [ {name, github_token, github_repo, github_branch, github_filename, last_upload} ] }
    return load_json_data(GITHUB_CONFIG_FILE, {})

def save_github_configs(data: dict):
    return save_json_data(GITHUB_CONFIG_FILE, data)


# -------- UPDATED /setgithub --------
# SAME NAME  -> overwrite
# NEW NAME   -> add (multiple allowed)
async def set_github_direct(update: Update, context: CallbackContext) -> None:
    user = update.effective_user
    if not user:
        return

    if not is_user_vip(user.id):
        await update.message.reply_text("‚ùå This command is VIP only.")
        return

    args = context.args
    if len(args) != 5:
        await update.message.reply_text(
            "Usage:\n/setgithub <name> <token> <owner/repo> <branch> <file.json>"
        )
        return

    name, token, repo, branch, filename = args
    uid = str(user.id)

    data = load_github_configs()
    configs = data.setdefault(uid, [])

    new_cfg = {
        "name": name,
        "github_token": token,
        "github_repo": repo,
        "github_branch": branch,
        "github_filename": filename,
        "last_upload": None
    }

    for cfg in configs:
        if cfg["name"] == name:
            cfg.update(new_cfg)   # overwrite ONLY same name
            save_github_configs(data)
            await update.message.reply_text(f"‚ôªÔ∏è GitHub config '{name}' updated.")
            return

    configs.append(new_cfg)       # add new
    save_github_configs(data)
    await update.message.reply_text(f"‚úÖ GitHub config '{name}' added.")


# -------- LIST USER GITHUB FILES --------
async def mygithub(update: Update, context: CallbackContext):
    uid = str(update.effective_user.id)
    data = load_github_configs()
    cfgs = data.get(uid, [])

    if not cfgs:
        await update.message.reply_text("‚ÑπÔ∏è No GitHub files configured.")
        return

    text = "üîó *Your GitHub Files*\n\n"
    for i, c in enumerate(cfgs, 1):
        text += f"{i}. `{c['name']}` ‚Üí {c['github_repo']} / {c['github_filename']}\n"

    await update.message.reply_text(text, parse_mode=ParseMode.MARKDOWN)


# -------- MULTI UPLOAD (used by existing process logic) --------
async def upload_all_github(bot, user_id, jwt_file):
    data = load_github_configs()
    cfgs = data.get(str(user_id), [])
    for cfg in cfgs:
        await upload_to_github_background(bot, user_id, jwt_file, cfg)


# ===================== STICKY MENU =====================
MAIN_MENU = ReplyKeyboardMarkup(
    [
        ["üì§ Process File", "‚è∞ Set File"],
        ["üîó My GitHub", "üíé VIP Status"],
        ["üëë Admin Panel"]
    ],
    resize_keyboard=True
)


async def show_menu(update: Update, context: CallbackContext):
    await update.message.reply_text("üìå Menu Active", reply_markup=MAIN_MENU)


# ===================== ADMIN PANEL =====================
ADMIN_MENU = ReplyKeyboardMarkup(
    [
        ["‚ûï Add VIP", "‚ûñ Remove VIP"],
        ["üìä All Users", "üîó All GitHub Sets"],
        ["‚¨Ö Back"]
    ],
    resize_keyboard=True
)


async def admin_panel(update: Update, context: CallbackContext):
    if update.effective_user.id != ADMIN_ID:
        return
    await update.message.reply_text(
        "üëë ADMIN CONTROL PANEL",
        reply_markup=ADMIN_MENU
    )

# ===============================================================================
